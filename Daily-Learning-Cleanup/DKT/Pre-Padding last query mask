{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Pre-Padding last query mask","provenance":[],"collapsed_sections":["SZwNyU_4V8D8"],"toc_visible":true,"authorship_tag":"ABX9TyMrou982w+Nw01LSRM5ti1C"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"SZwNyU_4V8D8"},"source":["# Pre-Padding Masking\n","\n","- mask 함수\n","\n","``` python\n","def get_mask(self, seq_len, mask, batch_size):\n","    new_mask = torch.zeros_like(mask)\n","    new_mask[mask == 0] = 1\n","    new_mask[mask != 0] = 0\n","    mask = new_mask\n","\n","    # batchsize * n_head 수만큼 각 mask를 반복하여 증가시킨다\n","    mask = mask.repeat(1, self.args.n_heads).view(batch_size*self.args.n_heads, -1, seq_len)\n","    return mask.masked_fill(mask==1, float('-inf'))\n","```\n","\n","- attention에 mask넣어주기\n","\n","```python\n","# before\n","out, _ = self.attn(q, k, v)\n","\n","# after\n","self.mask = self.get_mask(seq_len, mask, batch_size).to(self.device)\n","out, _ = self.attn(q, k, v, attn_mask=self.mask)\n","```"]},{"cell_type":"code","metadata":{"id":"zc89enm5u0kU"},"source":["class Feed_Forward_block(nn.Module):\n","    \"\"\"\n","    out =  Relu( M_out*w1 + b1) *w2 + b2\n","    \"\"\"\n","    def __init__(self, dim_ff):\n","        super().__init__()\n","        self.layer1 = nn.Linear(in_features=dim_ff, out_features=dim_ff)\n","        self.layer2 = nn.Linear(in_features=dim_ff, out_features=dim_ff)\n","\n","    def forward(self,ffn_in):\n","        return self.layer2(F.relu(self.layer1(ffn_in)))\n","\n","class LastQuery(nn.Module):\n","    def __init__(self, args):\n","        super(LastQuery, self).__init__()\n","        self.args = args\n","        self.device = args.device\n","\n","        self.hidden_dim = self.args.hidden_dim\n","        \n","        # Embedding \n","        # interaction은 현재 correct으로 구성되어있다. correct(1, 2) + padding(0)\n","        self.embedding_interaction = nn.Embedding(3, self.hidden_dim//3)\n","        self.embedding_test = nn.Embedding(self.args.n_test + 1, self.hidden_dim//3)\n","        self.embedding_question = nn.Embedding(self.args.n_questions + 1, self.hidden_dim//3)\n","        self.embedding_tag = nn.Embedding(self.args.n_tag + 1, self.hidden_dim//3)\n","        self.embedding_position = nn.Embedding(self.args.max_seq_len, self.hidden_dim)\n","\n","        # embedding combination projection\n","        self.comb_proj = nn.Linear((self.hidden_dim//3)*4, self.hidden_dim)\n","\n","        # 기존 keetar님 솔루션에서는 Positional Embedding은 사용되지 않습니다\n","        # 하지만 사용 여부는 자유롭게 결정해주세요 :)\n","        # self.embedding_position = nn.Embedding(self.args.max_seq_len, self.hidden_dim)\n","        \n","        # Encoder\n","        self.query = nn.Linear(in_features=self.hidden_dim, out_features=self.hidden_dim)\n","        self.key = nn.Linear(in_features=self.hidden_dim, out_features=self.hidden_dim)\n","        self.value = nn.Linear(in_features=self.hidden_dim, out_features=self.hidden_dim)\n","\n","        self.attn = nn.MultiheadAttention(embed_dim=self.hidden_dim, num_heads=self.args.n_heads)\n","        self.mask = None # last query에서는 필요가 없지만 수정을 고려하여서 넣어둠\n","        self.ffn = Feed_Forward_block(self.hidden_dim)      \n","\n","        self.ln1 = nn.LayerNorm(self.hidden_dim)\n","        self.ln2 = nn.LayerNorm(self.hidden_dim)\n","\n","        # LSTM\n","        self.lstm = nn.LSTM(\n","            self.hidden_dim,\n","            self.hidden_dim,\n","            self.args.n_layers,\n","            batch_first=True)\n","\n","        # Fully connected layer\n","        self.fc = nn.Linear(self.hidden_dim, 1)\n","       \n","        self.activation = nn.Sigmoid()\n","\n","    def get_pos(self, seq_len):\n","        # use sine positional embeddinds\n","        return torch.arange(seq_len).unsqueeze(0)\n"," \n","    def init_hidden(self, batch_size):\n","        h = torch.zeros(\n","            self.args.n_layers,\n","            batch_size,\n","            self.args.hidden_dim)\n","        h = h.to(self.device)\n","\n","        c = torch.zeros(\n","            self.args.n_layers,\n","            batch_size,\n","            self.args.hidden_dim)\n","        c = c.to(self.device)\n","\n","        return (h, c)\n","\n","\n","    def get_mask(self, seq_len, mask, batch_size):\n","        new_mask = torch.zeros_like(mask)\n","        new_mask[mask == 0] = 1\n","        new_mask[mask != 0] = 0\n","        mask = new_mask\n","    \n","        # batchsize * n_head 수만큼 각 mask를 반복하여 증가시킨다\n","        mask = mask.repeat(1, self.args.n_heads).view(batch_size*self.args.n_heads, -1, seq_len)\n","        return mask.masked_fill(mask==1, float('-inf'))\n","\n","\n","    def forward(self, input):\n","        test, question, tag, _, mask, interaction, index = input\n","        batch_size = interaction.size(0)\n","        seq_len = interaction.size(1)\n","\n","        # 신나는 embedding\n","        embed_interaction = self.embedding_interaction(interaction)\n","        embed_test = self.embedding_test(test)\n","        embed_question = self.embedding_question(question)\n","        embed_tag = self.embedding_tag(tag)\n","\n","        embed = torch.cat([embed_interaction,\n","                           embed_test,\n","                           embed_question,\n","                           embed_tag,], 2)\n","\n","        embed = self.comb_proj(embed)\n","\n","        # Positional Embedding\n","        # last query에서는 positional embedding을 하지 않음\n","        # position = self.get_pos(seq_len).to('cuda')\n","        # embed_pos = self.embedding_position(position)\n","        # embed = embed + embed_pos\n","\n","        ####################### ENCODER #####################\n","        q = self.query(embed)[:, -1:, :].permute(1, 0, 2)\n","        k = self.key(embed).permute(1, 0, 2)\n","        v = self.value(embed).permute(1, 0, 2)\n","\n","        ## attention\n","        # last query only\n","        self.mask = self.get_mask(seq_len, mask, batch_size).to(self.device)\n","        out, _ = self.attn(q, k, v, attn_mask=self.mask)\n","        \n","        ## residual + layer norm\n","        out = out.permute(1, 0, 2)\n","        out = embed + out\n","        out = self.ln1(out)\n","\n","        ## feed forward network\n","        out = self.ffn(out)\n","\n","        ## residual + layer norm\n","        out = embed + out\n","        out = self.ln2(out)\n","\n","        ###################### LSTM #####################\n","        hidden = self.init_hidden(batch_size)\n","        out, hidden = self.lstm(out, hidden)\n","\n","        ###################### DNN #####################\n","        out = out.contiguous().view(batch_size, -1, self.hidden_dim)\n","        out = self.fc(out)\n","\n","        preds = self.activation(out).view(batch_size, -1)\n","\n","        return preds\n"],"execution_count":null,"outputs":[]}]}