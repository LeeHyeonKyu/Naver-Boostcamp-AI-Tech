{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End to End 프로젝트 실습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 단계별로 각각의 작업들 확인\n",
    "터미널을 연뒤에 현재 디렉토리로 이동 하고 다음 명령어들을 실행해봅니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#혹시 기존에 6006포트를 사용하고 있는 프로세스가 있다면 다음 명령어로 종료를 해주겠습니다.\n",
    "#kill -9 $(lsof -t -i:6006)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1. 환경변수 설정\n",
    "- .bash_profile이나 .bash_rc 같은 곳에 영구적으로 설정 할 수 있으나, 저희는 새로운 터미널을 열때마다 직접 설정을 해주겠습니다.\n",
    "- 현재 디렉토리가 daily_mission_airflow 인 상태에서 다음 명령어를 입력합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export WORKING_DIRECTORY=$(pwd)\n",
    "# export AIRFLOW_HOME=~/airflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2. 기존의 훈련데이터를 데이터 베이스(sqlite)에 적재\n",
    "- 이 부분은 최초 1회만 실행됩니다\n",
    "- 실제 서비스에서는 MySQL과 같은 RDB나 NoSQL 계열의 진짜 DB를 사용하지만, 추가적인 환경설정에 힘을 빼지 않기 위해 저희는 [sqlite](https://en.wikipedia.org/wiki/SQLite)를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python file_to_db.py --path ${WORKING_DIRECTORY}/event_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-3. 데이터를 DB에서 추출해서 csv형태로 저장 (학습을 위한 데이터)\n",
    "이때, 데이터가 지속적으로 유입된다는 가상의 시나리오를 위해 한번 실행시 10000행씩 추출을 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python ${WORKING_DIRECTORY}/db_to_file.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-4. 훈련\n",
    "- 위에서 준비된 새로운 데이터 파일을 기반으로 훈련을 진행합니다\n",
    "- 기존의 train_data.csv가 아니고, 지속적으로 유입되는 10000개의 데이터라고 가정을 하기 위함입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# python ${WORKING_DIRECTORY}/dkt/train.py --model_dir ${WORKING_DIRECTORY}/models --asset_dir ${WORKING_DIRECTORY}/asset --data_dir ${WORKING_DIRECTORY} --file_name data.csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wandb에서 현재 훈련과정이 정상적으로 기록됐는지 확인해주세요!\n",
    "단, 소량의 데이터로만 훈련을 진행해서 무의미한 acc, auc가 나올 수 있습니다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-5. 서버실행\n",
    "- 우선 웹서비스 자체가 잘 동작하는지 다음 명령어로 확인해보겠습니다.\n",
    "- 일단 처음엔는 airflow GUI를 확인해야 하기 때문에, 외부에서 접근 가능하지 않은 포트로 일단 돌려 놓기만 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python ${WORKING_DIRECTORY}/server/server.py --port 6007"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-6. 서버 재시작\n",
    "새로운 모델 파일로 다시 서버를 로딩해야하는 경우, 정석적으로 서버를 운영할때는 시스템 데몬 형태로 운영하여서, 해당 데몬을 재부팅 하는 식으로 진행하지만, 저희는 flask에서 제공하는 debug기능을 활용하여 약간의 트릭을 사용하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#touch -m ${WORKING_DIRECTORY}/server/server.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Airflow 셋팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#이미 세팅을 하신 경우에는 생략하셔도 됩니다!\n",
    "!pip uninstall sqlalchemy -y\n",
    "!pip install 'sqlalchemy < 1.4.0' apache-airflow attrdict mlflow\n",
    "!airflow db init\n",
    "!mkdir ~/airflow/dags\n",
    "#GUI를 위한 유저생성\n",
    "\n",
    "!airflow users create \\\n",
    "    --username admin \\\n",
    "    --firstname Peter \\\n",
    "    --lastname Parker \\\n",
    "    --password 1234 \\\n",
    "    --role Admin \\\n",
    "    --email spiderman@superhero.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#터미널을 하나 켜서 다음 명령어 입력\n",
    "# export TZ=Asia/Seoul\n",
    "# export AIRFLOW_HOME=~/airflow\n",
    "# export WORKING_DIRECTORY=$(pwd)\n",
    "# airflow scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#터미널을 하나 더 켜서 다음 명령어 입력 (Airflow GUI 실행)\n",
    "# export TZ=Asia/Seoul\n",
    "# export AIRFLOW_HOME=~/airflow\n",
    "# export WORKING_DIRECTORY=$(pwd)\n",
    "# airflow webserver -p 6006\n",
    "\n",
    "# 이제 다음 주소로 접근 가능합니다 (텐서보드 접속 포트를 확인해주세요!)\n",
    "# http://<SERVER_IP>:6009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ~/airflow/dags/dkt_pipeline.py\n",
    "from datetime import timedelta\n",
    "from airflow import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "\n",
    "from airflow.utils.dates import days_ago\n",
    "\n",
    "\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'Peter Parker',\n",
    "    'email': ['example@example.com'],\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 2,\n",
    "    'retry_delay': timedelta(minutes=2),\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dag = DAG(\n",
    "    'dkt_pipeline',\n",
    "    default_args=default_args,\n",
    "    description='DKT workflow management',\n",
    "    start_date=days_ago(0),\n",
    "    schedule_interval='*/1 * * * *',\n",
    "    is_paused_upon_creation=False,\n",
    "    catchup = False,\n",
    "    max_active_runs=1\n",
    ")\n",
    "\n",
    "dataload = BashOperator(\n",
    "    task_id='dataload',\n",
    "    bash_command='python ${WORKING_DIRECTORY}/db_to_file.py',\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "train = BashOperator(\n",
    "    task_id='train',\n",
    "    bash_command='python ${WORKING_DIRECTORY}/dkt/train.py \\\n",
    "                    --model_dir ${WORKING_DIRECTORY}/models \\\n",
    "                    --asset_dir ${WORKING_DIRECTORY}/asset \\\n",
    "                    --data_dir ${WORKING_DIRECTORY} --file_name data.csv',\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "reload = BashOperator(\n",
    "    task_id='reload',\n",
    "    bash_command=\"touch -m ${WORKING_DIRECTORY}/server/server.py\",\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "\n",
    "dataload >> train >> reload\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db를 reset 해주고 터미널로가서 scheduler를 다시 실행합니다.\n",
    "!airflow db reset -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 입력데이터가 반영된 재훈련 확인\n",
    "airflow가 정상적으로 작업을 하는 것을 확인했으면, 이제 airflow webserver를 종료하고, 웹사이트의 포트를 변경한 뒤에 다시 여러분들의 문제풀이 데이터를 입력해주세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python ${WORKING_DIRECTORY}/server/server.py --port 6006"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
