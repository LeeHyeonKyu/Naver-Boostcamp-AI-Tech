{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Airflow, Mlflow를 활용한 ML Cycle\n",
    "\n",
    "## MNIST 손글씨 분석을 위한 데이터 수집-모델 훈련-배포 과정 구축"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 관련 패키지 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sqlalchemy의 버전이 1.4 이상인 경우 에러가 발생합니다.\n",
    "!pip uninstall sqlalchemy -y\n",
    "!pip install 'sqlalchemy < 1.4.0' apache-airflow attrdict mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Airflow 셋팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!airflow db init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본적으로 제공되는 예제 DAG들을 가리기 위해\n",
    "# 터미널에 다음과 같이 이동하여 해당 파일에 load example 옵션을 비활성화합니다.\n",
    "# (반드시 해줘야 하는 것은 아닙니다.)\n",
    "\n",
    "# cd ~/airflow\n",
    "# vim airflow.cfg\n",
    "\n",
    "## === airflow.cfg === ##\n",
    "# load_examples = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ~/airflow/dags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 멀티 프로세싱 관련된 에러를 피하기 위해\n",
    "# https://stackoverflow.com/questions/50168647/multiprocessing-causes-python-to-crash-and-gives-an-error-may-have-been-in-progr\n",
    "!export OBJC_DISABLE_INITIALIZE_FORK_SAFETY=YES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GUI를 위한 유저생성\n",
    "\n",
    "!airflow users create \\\n",
    "    --username admin \\\n",
    "    --firstname Peter \\\n",
    "    --lastname Parker \\\n",
    "    --password 1234 \\\n",
    "    --role Admin \\\n",
    "    --email spiderman@superhero.org\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#터미널을 하나 켜서 다음 명령어 입력\n",
    "# export TZ=Asia/Seoul\n",
    "# export AIRFLOW_HOME=~/airflow\n",
    "# airflow scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미 6006번 포트를 잡고 있을 경우를 대비해 6006번 포트를 사용하는 프로세스를 종료해줍니다.\n",
    "!apt-get install lsof\n",
    "!kill -9 `lsof -t -i:6006`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#터미널을 하나 더 켜서 다음 명령어 입력 (Airflow GUI 실행)\n",
    "# export TZ=Asia/Seoul\n",
    "# export AIRFLOW_HOME=~/airflow\n",
    "# airflow webserver -p 6006\n",
    "\n",
    "# 이제 다음 주소로 접근 가능합니다 (텐서보드 접속 포트를 확인해주세요!)\n",
    "# http://<SERVER_IP>:6009"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Airflow DAG 테스트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 간단한 Bash Operator를 기반으로 한 Dag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%writefile ~/airflow/dags/simple_dag.py\n",
    "from airflow import DAG\n",
    "from airflow.utils.dates import days_ago\n",
    "from airflow.operators.bash import BashOperator\n",
    "\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'Boost Kim',\n",
    "    'start_date': days_ago(1),\n",
    "}\n",
    "\n",
    "\n",
    "dag = DAG(\n",
    "    'simple_pipeline',\n",
    "    default_args=default_args,\n",
    "    description='A simple pipeline',\n",
    "    schedule_interval=None,\n",
    ")\n",
    "\n",
    "\n",
    "task_1 = BashOperator(\n",
    "    task_id='task_1',\n",
    "    bash_command=\"echo 1\",\n",
    "    dag=dag\n",
    ")\n",
    "task_2 = BashOperator(\n",
    "    task_id='task_3',\n",
    "    bash_command=\"echo 2\",\n",
    "    dag=dag\n",
    ")\n",
    "task_3 = BashOperator(\n",
    "    task_id='task_2',\n",
    "    bash_command=\"echo 3\",\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "\n",
    "task_1 >> task_2 >> task_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!airflow db reset -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 간단한 Python Operator 기반으로 한 Dag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ~/airflow/dags/python_dag.py\n",
    "from airflow import DAG\n",
    "from airflow.utils.dates import days_ago\n",
    "from airflow.operators.python import PythonOperator\n",
    "import time\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'Boost Kim',\n",
    "    'start_date': days_ago(1),\n",
    "}\n",
    "\n",
    "\n",
    "dag = DAG(\n",
    "    'simple_python_pipeline',\n",
    "    default_args=default_args,\n",
    "    description='A simple python pipeline',\n",
    "    schedule_interval=None,\n",
    ")\n",
    "\n",
    "#Python 함수에 변수를 받을 때는 *args = [], *kwargs = {} 형태로 받을 수 있다.\n",
    "def sleep(**kwargs):\n",
    "    delta = kwargs['delta']\n",
    "    time.sleep(delta)\n",
    "    print(\"Slept for {} seconds\".format(delta))\n",
    "    \n",
    "\n",
    "task_1 = PythonOperator(\n",
    "    task_id='task_1',\n",
    "    python_callable=sleep,\n",
    "    op_kwargs={'delta': 10}, #kwargs 형태로 전달\n",
    "    dag=dag\n",
    ")\n",
    "task_2 = PythonOperator(\n",
    "    task_id='task_2',\n",
    "    python_callable=sleep,\n",
    "    op_kwargs={'delta': 30}, #kwargs 형태로 전달\n",
    "    dag=dag\n",
    ")\n",
    "task_3 = PythonOperator(\n",
    "    task_id='task_3',\n",
    "    python_callable=sleep,\n",
    "    op_kwargs={'delta': 5}, #kwargs 형태로 전달\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "task_1 >> task_2 >> task_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!airflow db reset -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ~/airflow/dags/python_combined_dag.py\n",
    "# 이 실습에서는 database backend를 sqlite를 사용하기 때문에 병렬 처리가 되지 않지만, \n",
    "# mysql과 같은 데이터베이스를 사용하면 병렬 처리가 가능합니다.\n",
    "from airflow import DAG\n",
    "from airflow.utils.dates import days_ago\n",
    "from airflow.operators.python import PythonOperator\n",
    "import time\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'Boost Kim',\n",
    "    'start_date': days_ago(1),\n",
    "}\n",
    "\n",
    "\n",
    "dag = DAG(\n",
    "    'combined_python_pipeline',\n",
    "    default_args=default_args,\n",
    "    description='A combined python pipeline',\n",
    "    schedule_interval=None,\n",
    "    concurrency = 2\n",
    ")\n",
    "\n",
    "def sleep(**kwargs):\n",
    "    delta = kwargs['delta']\n",
    "    time.sleep(delta)\n",
    "    print(\"Slept for {} seconds\".format(delta))\n",
    "    \n",
    "\n",
    "task_1 = PythonOperator(\n",
    "    task_id='task_1',\n",
    "    python_callable=sleep,\n",
    "    op_kwargs={'delta': 10},\n",
    "    dag=dag\n",
    ")\n",
    "task_2_1 = PythonOperator(\n",
    "    task_id='task_2_1',\n",
    "    python_callable=sleep,\n",
    "    op_kwargs={'delta': 30},\n",
    "    dag=dag\n",
    ")\n",
    "task_2_2 = PythonOperator(\n",
    "    task_id='task_2_2',\n",
    "    python_callable=sleep,\n",
    "    op_kwargs={'delta': 10},\n",
    "    dag=dag\n",
    ")\n",
    "task_3 = PythonOperator(\n",
    "    task_id='task_3',\n",
    "    python_callable=sleep,\n",
    "    op_kwargs={'delta': 5},\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "# task_2_1과 task_2_2가 만족이 되어야 task_3dl tlfgod\n",
    "# 현재는 SequentialExecutor 세팅이지만, mysql, postresql 등을 연동하여 LocalExecutor 사용시 병렬 처리 가능!\n",
    "task_1 >> [task_2_1, task_2_2] >> task_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!airflow db reset -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 스케쥴을 사용한 Dag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ~/airflow/dags/scheduled_dag.py\n",
    "from airflow import DAG\n",
    "from airflow.utils.dates import days_ago\n",
    "from airflow.operators.bash import BashOperator\n",
    "\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'Boost Kim',\n",
    "    'start_date': days_ago(1),\n",
    "}\n",
    "\n",
    "# interval 세팅은 다음 링크 참고\n",
    "# https://airflow.apache.org/docs/apache-airflow/1.10.1/scheduler.html#dag-runs\n",
    "# 아래 예제는 매 5분마다\n",
    "dag = DAG(\n",
    "    'simple_scheduled_pipeline',\n",
    "    default_args=default_args,\n",
    "    description='A simple scheduled pipeline',\n",
    "    schedule_interval='*/5 * * * *',\n",
    ")\n",
    "\n",
    "\n",
    "task_1 = BashOperator(\n",
    "    task_id='task_1',\n",
    "    bash_command=\"echo 1\",\n",
    "    dag=dag\n",
    ")\n",
    "task_2 = BashOperator(\n",
    "    task_id='task_3',\n",
    "    bash_command=\"echo 2\",\n",
    "    dag=dag\n",
    ")\n",
    "task_3 = BashOperator(\n",
    "    task_id='task_2',\n",
    "    bash_command=\"echo 3\",\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "\n",
    "task_1 >> task_2 >> task_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!airflow db reset -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 한번에 한 run만 실행이 되고, 지난 시간 부분을 채우지 않는 Dag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ~/airflow/dags/strict_scheduled_dag.py\n",
    "from airflow import DAG\n",
    "from airflow.utils.dates import days_ago\n",
    "from airflow.operators.bash import BashOperator\n",
    "\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'Boost Kim',\n",
    "    'start_date': days_ago(1),\n",
    "}\n",
    "\n",
    "# 1분 마다 하되, scheduler에 등록된 시점을 기준으로만, 그리고 한번에 최대 1개의 run만\n",
    "dag = DAG(\n",
    "    'strict_scheduled_pipeline',\n",
    "    default_args=default_args,\n",
    "    description='A strict scheduled pipeline',\n",
    "    schedule_interval='*/1 * * * *',\n",
    "    is_paused_upon_creation=False, #등록되면 바로 활성화\n",
    "    catchup = False, # 시작시점(start_date) 부터 채워 넣지 않기\n",
    "    max_active_runs=1 #한번에 한 run만\n",
    ")\n",
    "\n",
    "\n",
    "task_1 = BashOperator(\n",
    "    task_id='task_1',\n",
    "    bash_command=\"echo 1\",\n",
    "    dag=dag\n",
    ")\n",
    "task_2 = BashOperator(\n",
    "    task_id='task_3',\n",
    "    bash_command=\"echo 2\",\n",
    "    dag=dag\n",
    ")\n",
    "task_3 = BashOperator(\n",
    "    task_id='task_2',\n",
    "    bash_command=\"echo 3\",\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "\n",
    "task_1 >> task_2 >> task_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!airflow db reset -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 오늘의 실제 실습 Dag\n",
    "- 강의용 코드기 때문에 한 파일에 관련된 함수들을 모두 다 넣었습니다.\n",
    "- 아래 코드는 실행용이 아닌 airflow dags 폴더에 저장이 되도록해놓은 코드입니다.\n",
    "- 데이터 수집 - 훈련 - 배포"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ~/airflow/dags/airflow_example.py\n",
    "\n",
    "from datetime import timedelta\n",
    "from airflow import DAG\n",
    "\n",
    "from airflow.utils.dates import days_ago\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.operators.bash import BashOperator\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "import mlflow\n",
    "from attrdict import AttrDict\n",
    "\n",
    "import random\n",
    "import requests\n",
    "\n",
    "#####START ML CODE#####\n",
    "def collect_data():\n",
    "    #40%의 확률로 에러가 발생하도록 설정해놨습니다.\n",
    "    if random.randint(0,10) < 4:\n",
    "        raise Exception(\"Fake ERROR: Failed to Download!\")\n",
    "    url = 'https://s3.amazonaws.com/img-datasets/mnist.npz'\n",
    "    r = requests.get(url, allow_redirects=True)\n",
    "    open('mnist.npz', 'wb').write(r.content)\n",
    "    \n",
    "# context는 operator간의 값들을 공유하기 위함\n",
    "def train(**context):\n",
    "    config = AttrDict(context['dag_run'].conf)\n",
    "\n",
    "    mlflowInit(config)\n",
    "    f = np.load('mnist.npz')\n",
    "\n",
    "    sample = 5000\n",
    "    X_train, y_train = f['x_train'][:sample], f['y_train'][:sample]\n",
    "    X_test, y_test = f['x_test'], f['y_test']\n",
    "\n",
    "    X_train, X_test = X_train.reshape(X_train.shape[0],-1), X_test.reshape(X_test.shape[0],-1)\n",
    "    \n",
    "\n",
    "    clf = RandomForestClassifier()\n",
    "    clf.fit(X_train, y_train)\n",
    "    prediction = clf.predict(X_test)\n",
    "    result = (prediction == y_test).mean()\n",
    "    print(result)\n",
    "    mlflow.log_metric('acc',result)\n",
    "    \n",
    "    mlflow.sklearn.log_model(clf, 'save_model')\n",
    "    model_path = mlflow.get_artifact_uri().replace('file://', '')\n",
    "    \n",
    "    #중요: task간의 값들을 전달하는 방법\n",
    "    #xcom == cross communication\n",
    "    context['ti'].xcom_push(key='model_path', value=model_path)\n",
    "    \n",
    "    mlflow.end_run()\n",
    "\n",
    "def mlflowInit(config):\n",
    "    try:\n",
    "        mlflow.create_experiment(name=config.experiment)\n",
    "    except:\n",
    "        print('Exist experiment')\n",
    "\n",
    "    mlflow.set_experiment(config.experiment)\n",
    "\n",
    "    mlflow.start_run()\n",
    "\n",
    "    mlflow.set_tag('version', config.version)\n",
    "    mlflow.log_params(config)\n",
    "#####END ML CODE#####\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#####START DAG CODE#####\n",
    "default_args = {\n",
    "    'owner': 'Boost Kim',\n",
    "    'depends_on_past': True,\n",
    "    'start_date': days_ago(1),\n",
    "    'retries': 4,\n",
    "    'retry_delay': timedelta(seconds=20)\n",
    "}\n",
    "\n",
    "\n",
    "dag = DAG(\n",
    "    'ml_pipeline',\n",
    "    default_args=default_args,\n",
    "    description='A simple Machine Learning pipeline',\n",
    "    schedule_interval=None,\n",
    ")\n",
    "\n",
    "\n",
    "download_images = PythonOperator(\n",
    "    task_id='collect_data',\n",
    "    python_callable=collect_data,\n",
    "    retries=3,\n",
    "    dag=dag,\n",
    ")\n",
    "train = PythonOperator(\n",
    "    task_id='train',\n",
    "    depends_on_past=True,\n",
    "    python_callable=train,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "#airflow ti 변수 활용 (jinja tempalte 방식)\n",
    "#https://airflow.apache.org/docs/apache-airflow/stable/macros-ref.html#macros-reference\n",
    "serve = BashOperator(\n",
    "    task_id='serve',\n",
    "    depends_on_past=False,\n",
    "    bash_command=\"mlflow models serve -m {{ ti.xcom_pull(key='model_path') }}/save_model --no-conda -h 0.0.0.0 -p 8889 &\",\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "download_images >> train >> serve\n",
    "\n",
    "#####END DAG CODE#####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!airflow db reset -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DAG 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#airflow GUI 상에서 해당 DAG를 trigger를 하면서 입력해주는 configuration\n",
    "#{\"version\": 0.1,\"experiment\": \"mlflow-airflow\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 서빙되는 모델 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='data/mnist_5.jpg') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from PIL import Image\n",
    "import requests, json\n",
    "import numpy as np\n",
    "\n",
    "img = Image.open('data/mnist_5.jpg')\n",
    "image_data = np.array(img, dtype='uint8').reshape(-1).tolist()\n",
    "\n",
    "url = 'http://localhost:8889/invocations'\n",
    "\n",
    "data = {\n",
    "    \"columns\": [i for i in range(0, len(image_data))],\n",
    "    \"data\": [image_data]\n",
    "}\n",
    "headers = {\n",
    "    'content-type':'application/json'\n",
    "}\n",
    "res = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "\n",
    "print('Predicted From Server:',json.loads(res.text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#백그라운드로 돌고있는 웹서버를 종료하기 위한 코드\n",
    "!apt-get install lsof\n",
    "!kill -9 `lsof -t -i:8889`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
