{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Day8_실습자료_0_한국어_GPT_2_pre_training.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"YrzmQff3nU6z"},"source":["# GPT-2 학습해보기\n","\n","> 작성자      \n","```\n","* 김성현 (bananaband657@gmail.com)  \n","김바다 (qkek983@gmail.com)\n","박상희 (parksanghee0103@gmail.com)  \n","이정우 (jungwoo.l2.rs@gmail.com)\n","```\n","[CC BY-NC-ND](https://creativecommons.org/licenses/by-nc-nd/2.0/kr/)"]},{"cell_type":"markdown","metadata":{"id":"SmoeUd5qnaq9"},"source":["이번 시간엔 한국어 코퍼스를 활용해, 직접 한국어 GPT-2를 학습해보겠습니다."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"id8FcYRa48Gc","executionInfo":{"status":"ok","timestamp":1617978776452,"user_tz":-540,"elapsed":7800,"user":{"displayName":"바나나인간","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhCM2sOGSCeogj8b2W7aPl7KKywidts5H45gy6vCA=s64","userId":"05069217733258421588"}},"outputId":"c5259169-44d5-458d-cc0b-cfcf11443cf1"},"source":["!pip install transformers"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/81/91/61d69d58a1af1bd81d9ca9d62c90a6de3ab80d77f27c5df65d9a2c1f5626/transformers-4.5.0-py3-none-any.whl (2.1MB)\n","\u001b[K     |████████████████████████████████| 2.2MB 6.5MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/cd/342e584ee544d044fb573ae697404ce22ede086c9e87ce5960772084cad0/sacremoses-0.0.44.tar.gz (862kB)\n","\u001b[K     |████████████████████████████████| 870kB 26.4MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Collecting tokenizers<0.11,>=0.10.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n","\u001b[K     |████████████████████████████████| 3.3MB 33.2MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.44-cp37-none-any.whl size=886084 sha256=a4ef502623620acf04f19d8fdeae7015039cb6ede1618c4a4765d4ff30c46c7c\n","  Stored in directory: /root/.cache/pip/wheels/3e/fb/c0/13ab4d63d537658f448366744654323077c4d90069b6512f3c\n","Successfully built sacremoses\n","Installing collected packages: sacremoses, tokenizers, transformers\n","Successfully installed sacremoses-0.0.44 tokenizers-0.10.2 transformers-4.5.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lrihNAMK5EsC","executionInfo":{"status":"ok","timestamp":1617978778870,"user_tz":-540,"elapsed":9226,"user":{"displayName":"바나나인간","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhCM2sOGSCeogj8b2W7aPl7KKywidts5H45gy6vCA=s64","userId":"05069217733258421588"}},"outputId":"d571163c-0e2d-44fa-a130-c68b9973c28f"},"source":["import torch\n","torch.cuda.is_available()"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"xbfDhMRUC-OL"},"source":["역시 위키 데이터를 가져와볼까요?"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2UYtcoe0C9QT","executionInfo":{"status":"ok","timestamp":1617978756095,"user_tz":-540,"elapsed":3954,"user":{"displayName":"바나나인간","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhCM2sOGSCeogj8b2W7aPl7KKywidts5H45gy6vCA=s64","userId":"05069217733258421588"}},"outputId":"5fc2896f-547c-42c4-db0e-949cd7c010ae"},"source":["!mkdir my_data\n","!curl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id=1zib1GI8Q5wV08TgYBa2GagqNh4jyfXZz\" > /dev/null\n","!curl -Lb ./cookie \"https://drive.google.com/uc?export=download&confirm=`awk '/download/ {print $NF}' ./cookie`&id=1zib1GI8Q5wV08TgYBa2GagqNh4jyfXZz\" -o my_data/wiki_20190620_small.txt"],"execution_count":1,"outputs":[{"output_type":"stream","text":["  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100   408    0   408    0     0    532      0 --:--:-- --:--:-- --:--:--   531\n","  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n","  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n","100 1323k  100 1323k    0     0  1284k      0  0:00:01  0:00:01 --:--:--  167M\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kB-X8eCflkD-","executionInfo":{"status":"ok","timestamp":1617978758659,"user_tz":-540,"elapsed":477,"user":{"displayName":"바나나인간","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhCM2sOGSCeogj8b2W7aPl7KKywidts5H45gy6vCA=s64","userId":"05069217733258421588"}}},"source":["path = \"/content/my_data/wiki_20190620_small.txt\""],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uAIHClHAgz5U"},"source":["지금까지는 BertWordPieceTokenizer를 사용해왔다면,   \n","이번에는 SentencePiceBPETokenizer를 사용해 모델을 학습해보겠습니다.\n","\n","각 tokenizer의 차이는 허훈님의 블로그 [여기](https://huffon.github.io/2020/07/05/tokenizers/) 에서 확인하실 수 있습니다.\n","\n"]},{"cell_type":"code","metadata":{"id":"cC7g-nhTu0LN","executionInfo":{"status":"ok","timestamp":1617978783580,"user_tz":-540,"elapsed":1533,"user":{"displayName":"바나나인간","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhCM2sOGSCeogj8b2W7aPl7KKywidts5H45gy6vCA=s64","userId":"05069217733258421588"}}},"source":["from tokenizers import SentencePieceBPETokenizer\n","from tokenizers.normalizers import BertNormalizer\n","\n","tokenizer = SentencePieceBPETokenizer()\n","\n","tokenizer._tokenizer.normalizer = BertNormalizer(clean_text=True,\n","handle_chinese_chars=False,\n","lowercase=False)\n","\n","tokenizer.train(\n","    path,\n","    vocab_size=10000,\n","    special_tokens=[\n","        \"<s>\",\n","        \"<pad>\",\n","        \"</s>\",\n","        \"<unk>\",\n","    ],\n",")\n","\n"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"irrc7HKq8kpi","executionInfo":{"status":"ok","timestamp":1617978787584,"user_tz":-540,"elapsed":486,"user":{"displayName":"바나나인간","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhCM2sOGSCeogj8b2W7aPl7KKywidts5H45gy6vCA=s64","userId":"05069217733258421588"}},"outputId":"86cb8c24-16a4-41b9-f60e-7db58c9bc8fd"},"source":["print(tokenizer.encode(\"이순신은 조선 중기의 무신이다.\"))\n","print(tokenizer.encode(\"이순신은 조선 중기의 무신이다.\").ids)\n","print(tokenizer.encode(\"이순신은 조선 중기의 무신이다.\").tokens)\n","print(tokenizer.decode(tokenizer.encode(\"<s>이순신은 조선 중기의 무신이다.</s>\").ids, skip_special_tokens=True))\n","# SentencePiece를 사용하면, 나중에 decoding 과정에서 '_' 만 ' '로 replace해주면 띄어쓰기 복원이 가능해집니다."],"execution_count":7,"outputs":[{"output_type":"stream","text":["Encoding(num_tokens=9, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n","[1005, 577, 6613, 1303, 1041, 2071, 1136, 594, 1033]\n","['▁이', '순', '신은', '▁조선', '▁중', '기의', '▁무', '신', '이다.']\n","이순신은 조선 중기의 무신이다.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kQyn3mpR-YiI","outputId":"36686fea-be94-4f5f-b4cd-b07994899679"},"source":["tokenizer.save_model(\".\")"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['./vocab.json', './merges.txt']"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"hHbGAlyODPaB"},"source":["tokenizer = SentencePieceBPETokenizer.from_file(vocab_filename=\"vocab.json\", merges_filename=\"merges.txt\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p63PBpMZtNii","outputId":"2647d910-9e44-4b0a-f7b7-34f5dc7b303d"},"source":["print(tokenizer.encode(\"이순신은 조선 중기의 무신이다.\"))\n","print(tokenizer.encode(\"이순신은 조선 중기의 무신이다.\").ids)\n","print(tokenizer.encode(\"이순신은 조선 중기의 무신이다.\").tokens)\n","print(tokenizer.encode(\"<s>이순신은 조선 중기의 무신이다.</s>\").tokens)\n","print(tokenizer.decode(tokenizer.encode(\"<s>이순신은 조선 중기의 무신이다.</s>\").ids, skip_special_tokens=True))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Encoding(num_tokens=9, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n","[1005, 578, 6613, 1303, 1041, 2071, 1136, 595, 1033]\n","['▁이', '순', '신은', '▁조선', '▁중', '기의', '▁무', '신', '이다.']\n","['▁<', 's', '>', '이', '순', '신은', '▁조선', '▁중', '기의', '▁무', '신', '이다.', '<', '/s', '>']\n","<s>이순신은 조선 중기의 무신이다.</s>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Eyf_iqsnDa9-","outputId":"ba27102a-ccab-4872-df89-cf54afede345"},"source":["tokenizer.add_special_tokens([\"<s>\", \"</s>\", \"<unk>\", \"<pad>\", \"<shkim>\"])\n","tokenizer.pad_token_id = tokenizer.token_to_id(\"<pad>\")\n","tokenizer.unk_token_id = tokenizer.token_to_id(\"<unk>\")\n","tokenizer.bos_token_id = tokenizer.token_to_id(\"<bos>\")\n","tokenizer.eos_token_id = tokenizer.token_to_id(\"<eos>\")\n","\n","print(tokenizer.encode(\"<s>이순신은 조선 중기의 무신이다.</s>\").ids)\n","print(tokenizer.encode(\"<s>이순신은 조선 중기의 무신이다.</s>\").tokens)\n","print(tokenizer.decode(tokenizer.encode(\"<s>이순신은 조선 중기의 무신이다.</s>\").ids, skip_special_tokens=True))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[0, 1005, 578, 6613, 1303, 1041, 2071, 1136, 595, 1033, 2]\n","['<s>', '▁이', '순', '신은', '▁조선', '▁중', '기의', '▁무', '신', '이다.', '</s>']\n","이순신은 조선 중기의 무신이다.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VHWcg4ba7E-U"},"source":["from transformers import GPT2Config, GPT2LMHeadModel\n","# creating the configurations from which the model can be made\n","config = GPT2Config(\n","  vocab_size=tokenizer.get_vocab_size(),\n","  bos_token_id=tokenizer.token_to_id(\"<s>\"),\n","  eos_token_id=tokenizer.token_to_id(\"</s>\"),\n",")\n","# creating the model\n","model = GPT2LMHeadModel(config)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IhsrdEnd7Xur","outputId":"a88f33c2-58c4-4b89-bdb6-b7470a5ef254"},"source":["model.num_parameters()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["93522432"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"rMcqoMGE1p3A"},"source":["import json\n","import os\n","import pickle\n","import random\n","import time\n","import warnings\n","from typing import Dict, List, Optional\n","\n","import torch\n","from torch.utils.data.dataset import Dataset\n","\n","from filelock import FileLock\n","\n","from transformers.tokenization_utils import PreTrainedTokenizer\n","from transformers.utils import logging\n","\n","logger = logging.get_logger(__name__)\n","\n","class TextDataset(Dataset):\n","    \"\"\"\n","    This will be superseded by a framework-agnostic approach soon.\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        tokenizer: PreTrainedTokenizer,\n","        file_path: str,\n","        block_size: int,\n","        overwrite_cache=False,\n","        cache_dir: Optional[str] = None,\n","    ):\n","        assert os.path.isfile(file_path), f\"Input file path {file_path} not found\"\n","\n","        block_size = block_size - tokenizer.num_special_tokens_to_add(is_pair=False)\n","\n","        directory, filename = os.path.split(file_path)\n","        cached_features_file = os.path.join(\n","            cache_dir if cache_dir is not None else directory,\n","            \"cached_lm_{}_{}_{}\".format(\n","                tokenizer.__class__.__name__,\n","                str(block_size),\n","                filename,\n","            ),\n","        )\n","\n","        # Make sure only the first process in distributed training processes the dataset,\n","        # and the others will use the cache.\n","        lock_path = cached_features_file + \".lock\"\n","        with FileLock(lock_path):\n","\n","            if os.path.exists(cached_features_file) and not overwrite_cache:\n","                start = time.time()\n","                with open(cached_features_file, \"rb\") as handle:\n","                    self.examples = pickle.load(handle)\n","                logger.info(\n","                    f\"Loading features from cached file {cached_features_file} [took %.3f s]\", time.time() - start\n","                )\n","\n","            else:\n","                logger.info(f\"Creating features from dataset file at {directory}\")\n","                # 여기서부터 본격적으로 데이터셋을 만들기 시작합니다.\n","                self.examples = []\n","                text = \"\"\n","                with open(file_path, encoding=\"utf-8\") as f:\n","                    lines = f.readlines()\n","                    for line in lines:\n","                        line = line.strip()\n","                        line = \"<s>\"+line+\"</s>\" # 학습 데이터 앞 뒤에 문장 구분 기호를 추가해줍니다.\n","                        text += line    # 'text' 객체에 모든 학습 데이터를 다 합쳐버립니다 :-)\n","                tokenized_text = tokenizer.encode(text).ids\n","\n","                # 모델의 최대 sequence length만큼 데이터를 잘라서 저장합니다.\n","                for i in range(0, len(tokenized_text) - block_size + 1, block_size):  # Truncate in block of block_size\n","                    self.examples.append(\n","                        tokenized_text[i : i + block_size]\n","                    )\n","                # Note that we are losing the last truncated example here for the sake of simplicity (no padding)\n","                # If your dataset is small, first you should look for a bigger one :-) and second you\n","                # can change this behavior by adding (model specific) padding.\n","\n","                start = time.time()\n","                with open(cached_features_file, \"wb\") as handle:\n","                    pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","                logger.info(\n","                    \"Saving features into cached file %s [took %.3f s]\", cached_features_file, time.time() - start\n","                )\n","\n","    def __len__(self):\n","        return len(self.examples)\n","\n","    def __getitem__(self, i) -> torch.Tensor:\n","        return torch.tensor(self.examples[i], dtype=torch.long)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D3mSeU-40mmG"},"source":["dataset = TextDataset(\n","    tokenizer=tokenizer,\n","    file_path=path,\n","    block_size=128,\n",")\n","from transformers import DataCollatorForLanguageModeling\n","\n","data_collator = DataCollatorForLanguageModeling(    # GPT는 생성모델이기 때문에 [MASK] 가 필요 없습니다 :-)\n","    tokenizer=tokenizer, mlm=False,\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ghC6K4AX6UbY","outputId":"55004b91-e95c-43ef-8a4e-2869120573f6"},"source":["print(dataset[0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([   0, 3997, 3546, 8404,  464,    4, 5480, 9527, 1798, 1890, 2297, 1262,\n","        9626, 2679, 1188, 2174,    2,    0, 5708, 5480,  255, 6466,  750, 3426,\n","         872, 1556,  680,  895, 1626, 9223,  587, 3621, 1010, 3303,    2,    0,\n","        6466, 7418, 2305,  404, 2217, 1074,    2,    0, 1013, 1107, 3716,  646,\n","        8574, 1024,  940,   92, 7323,  372,   92,  721, 9295,  705, 1651,  454,\n","        3166, 1032, 1074,    2,    0, 6343, 1262, 3716, 1009, 2932, 1176,  913,\n","        2036, 1171, 3227,  843,   92,  440,  974, 1486, 1017,    3, 1323, 3914,\n","        2095, 1042,    2,    0, 1383, 2068, 2225, 1095,  327,  843, 1824,  507,\n","           4, 1240, 7698,    2,    0, 3897, 6466, 1053, 1077,  686, 2318, 4649,\n","        5204, 5671, 1013, 1759,  115, 2742, 3004,  104,  655, 2283, 9765, 1192,\n","        1796, 2449, 2546, 9939, 6466, 1053, 1037,  534])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AEGfg7JL7KWJ"},"source":["from transformers import Trainer, TrainingArguments\n","\n","training_args = TrainingArguments(\n","    output_dir='model_output',\n","    overwrite_output_dir=True,\n","    num_train_epochs=50,\n","    per_device_train_batch_size=64, # 512:32  # 128:64\n","    save_steps=1000,\n","    save_total_limit=2,\n","    logging_steps=100\n","\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    data_collator=data_collator,\n","    train_dataset=dataset\n",")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wwdMy08j7u-F"},"source":["trainer.train()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lpeIS3C2ipTZ"},"source":["trainer.save_model()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jxg4-f3P7zh2"},"source":["USE_GPU = 1\n","# Device configuration\n","device = torch.device('cuda' if (torch.cuda.is_available() and USE_GPU) else 'cpu')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XKGRHjzya13d","outputId":"5b881c98-3fc9-4e33-c4d2-c767c36b695a"},"source":["import torch\n","torch.manual_seed(42)\n","\n","input_ids = torch.tensor(tokenizer.encode(\"<s>이순신\", add_special_tokens=True).ids).unsqueeze(0).to('cuda')\n","\n","output_sequences = model.generate(input_ids=input_ids, do_sample=True, max_length=100, num_return_sequences=3)\n","for generated_sequence in output_sequences:\n","    generated_sequence = generated_sequence.tolist()\n","    print(\"GENERATED SEQUENCE : {0}\".format(tokenizer.decode(generated_sequence, skip_special_tokens=True)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"],"name":"stderr"},{"output_type":"stream","text":["GENERATED SEQUENCE : 이순신의 반은 인으로 데인 반라도 가장 많이 오자가 되었다.\n","GENERATED SEQUENCE : 이순신적인 것은 '사가 큰 도문에 비해 만들어 것을 하는 것이다.\n","GENERATED SEQUENCE : 이순신가 지 두 원을 시작은 그의 정아야체의 같은 리의 때문에 191인 때 전로 사용하여, 그는 \"용카키레한 후키에 의해 위해 “트를 \"카라 가져, 로예트를 고통되어 중국 비디오 대전할 수 없는 「스 바린 시작되어 여러 차례 5월인 독일·자한 자전적인 소설라키의 중국계리문이다.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xUL4-4v8IONB"},"source":[""],"execution_count":null,"outputs":[]}]}