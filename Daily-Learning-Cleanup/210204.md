## 강의 내용 정리

-   **RNN Tutorial**
-   **LSTM**
-   **Transformer**

##### RNN Tutorial

1.  Sequential Data  
    순차적인 정보가 유의미한 Data를 의미한다. 시간과 같은 일정한 순서에 따라 나열된 Data로 소리, 문자열, 주가 등이 대표적이다. 이들은 어떤 순서에 의해 종합된 Data이며, 특정 Step에서 값의 의미를 갖는다.  
    시퀀스 데이터는 독립동등분포의 가정을 쉽게 위배하기 때문에, 순서를 바꾸거나 과거 정보에 손실이 발생하면 데이터의 확률분포도 바뀌게 된다. 즉, 순차적인 정보에 대한 조작은 예측 등의 성능을 떨어뜨릴 수 있다.  
    순서를 유지한 채 시퀀스 데이터를 다루는 방법으로 조건부 확률을 사용할 수 있다. 이전 시퀀스의 정보를 가지고 앞으로 발생할 데이터의 확률분포를 예측하는 개념이다. 특정 시점인 t-Step에 대한 예측은 다음과 같이 정의될 수 있다.
    $$P(X_t) = P(X_t|X_{t-1},...X_1)P(X_{t-1},...X_1)$$  
    위의 식을 모든 Step에 대해 일반화하면, 다음과 같이 정리된다.  
    $$P(X_1,...X_t)=\prod_s^t{P(X_s|X_{s-1},...,X_1)}$$

2.  Sequential Data Handling  
    위의 식을 통해 t시점의 확률분포를 구할 수 있으나, 이를 모델링하는 것에는 어려움이 따른다. 조건부에 해당하는 Term의 길이가 가변적이기 때문이다. 이를 고정길이로 변화시키기 위해 t시점으로부터 $\tau$만큼 떨어진 일정한 거리를 상정할 수 있다.  
    위는 상황에 따라 매우 설득력있는 방법으로 사용될 수 있다. 어떤 회사의 주가를 예측하고자 할 때, 해당 회사의 상장시점부터 오늘까지의 데이터가 필요하지 않을 수 있기 때문이다.  
    고정된 길이 $\tau$만큼의 시퀀스만 사용하는 경우 **Autoregressive(AR) Model**이라고 부른다. AR Model 중 가장 간단한 것은 Markov Model로 $\tau$를 1로 설정한 모델을 의미한다.  

3.  Latent AR Model  
    Latent AR Model은 고정된 길이 $\tau$ 대신, 잠재변수인 $H_t$ Term을 추가한다. t시점의 $X_t$에 대해서 직전의 데이터인 $X_{t-1}$와 $H_t$ Term을 활용 하는 것이다.$H$ Term의 추가로 가변적인 길이에 대한 문제가 해결되며, 과거 데이터를 축적할 수 있다. $H$ 변수는 이전의 데이터들을 매번 저장하고 갱신하여 t시점마다 변화하는 값이 된다.  
    $$X_t \sim P(X_t|X_{t-1}, H_{t}) \\ H_t = Net_{\theta}(H_{t-1},X_{t-1})$$

4.  Basic RNN  
    잠재변수 $H_t$를 신경망을 통해 반복 사용해 시퀀스 데이터의 패턴을 학습하는 모델이 RNN이다. 가장 기본적인 RNN 모델은 MLP와 유사한 모양을 갖는다.  
    RNN은 이전 순서의 잠재변수와 현재의 입력을 활용해 모델링한다. 잠재변수인 $H_t$는 복제되어 t+1 Step의 출력과 잠재변수에 영향을 준다.  
    $$O_t = H_tW^{(2)}+b^{(2)} \\ H_t = \sigma{(X_tW_X^{(1)}+H_{t-1}W_H^{(1)}+b^{(1)})}$$  
    위 수식에서 각 가중치 행렬 $W$는 해당하는 값과 연산되는 것이며, Step과는 무관하게 매번 동일한 값이 연산되는 것이다. Step에 영향을 받는 것은 입력($X_t$)과 잠재변수($H_t$) 그리고 출력($O_t$)뿐이다.

5.  BPTT  
    RNN의 역전파는 가장 마지막의 Step의 출력으로부터 시작된다. 이 특징으로 RNN의 역전파는 Backpropagation Through Time (BPTT)라고 칭한다.  
    마지막 출력으로 생긴 Loss Function의 값을 미분하여 계산하게 되면, 시퀀스가 길수록 앞쪽의 데이터의 Grad에는 문제가 생기게 된다. 앞쪽의 데이터를 제대로 고려하지 못한 채 가중치가 최적화 될 수 있다는 의미이다.  
    RNN의 가중치 행렬을 미분하게 되면, 앞쪽의 데이터일수록 활성함수에 대한 미분이 연속해서 연산되게 되는데, 이는 Vanishing Grad 혹은 Exploding Grad의 이슈가 생기게 된다.  
    활성함수로 Sigmoid나 Tanh을 사용하게 되면 기울기가 점점 소실될 것이고, ReLU를 사용할 경우 값이 지수적으로 증가하여 문제가 생기게 될 것이다. (이와 같은 문제로 RNN에서는 ReLU를 사용하지 않는다.)

6.  Truncate BPTT  
    위와 같은 문제를 해결하기 위해 RNN을 Block처럼 구분하여 Grad를 계산하는 방법이 고안되었다. 특정 길이만큼 Block을 구성하여, 각 Block의 마지막 출력으로부터 Grad를 계산하는 것이다. 그럼에도 불구하고 시퀀스의 길이가 길어질 경우 제대로 된 학습이 보장되지 않으므로, LSTM과 GRU와 같은 Model이 고안되었다.  

##### LSTM  

1.  Sequential Data Problems  
    Sequential Data를 기반으로 하는 문제는 기존의 Data를 활용한 것과는 다른 성격을 지닌다. 시퀀스 데이터는 특성 상 입력의 차원을 한정지을 수 없고, 출력도 기존과 다른 성격을 지닌다.  
    이와 같은 문제를 풀기 위해 RNN과 같은 방법이 등장했고, RNN이 가진 한계를 넘어서기 위해 LSTM이 고안되었다.

2.  LSTM  
    LSTM은 Long Short Term Memory의 약자로 이름에 나와있듯 먼 과거의 데이터도 저장할 수 있는 모델이다. LSTM은 RNN의 한 종류로, 입력에 따라 각 Network가 매번 동작하는 형식이며, 이전의 입력을 특정 변수에 저장하여 활용한다.  
    LSTM은 3개의 Gate를 가지며, 입력 x에 대해 유의미한 정보만을 보관하는 Cell State와 이전의 출력값을 가지는 Hidden State로 구성되어 있다.  

3.  LSTM Core Idea  
    LSTM의 가장 핵심적인 아이디어는 Cell State이다. 들어오는 모든 입력에 대해 유의미한 정보만을 갖고 있는 것이다. 이를 위해 Forget Gate와 Input Gate가 존재한다. 이들은 매 입력마다 Cell State에서 불필요한 정보를 없애고, 필요한 정보만을 갱신한다.

4.  Forget Gate  
    Forget Gate는 입력 $x_t$와 이전 입력의 출력인 $h_{t-1}$을 이용해 특정 값을 생성해낸다. 불필요한 정보를 찾아서, 이 값을 Cell State에 전달한다.  
    $$f_t=\sigma{W_f[h_{t-1},x_t]+b_f}$$

5.  Input Gate  
    Input Gate는 Forget Gate와 동일한 입력을 통해 두개의 값을 생성해낸다. 이 두개의 값을 곱하여 유의미한 정보를 계산하고, 이 값을 Cell State에 전달한다.  
    $$i_t=\sigma{W_i[h_{t-1},x_t]+b_i} \\ \tilde{C}=tanh(W_C[h_{t-1},x_t]+b_C)$$

6.  Output Gate  
    Output Gate는 입력 $x_t$와 이전 입력의 출력인 $h_{t-1}$과 새롭게 갱신된 Cell State의 값을 입력받는다. 이들의 조합을 통해서 새로운 Output을 만들어 출력하며, 해당 값을 조작하고 Hidden State로 전달해 다음 입력에 사용할 수 있도록 갱신한다.  
    $$o_t=\sigma{W_o[h_{t-1},x_t]+b_o} \\ h_t=o_t*tanh(C_t)$$

7.  GRU  
    GRU는 LSTM 이후에 나온 모델로서, 2개의 게이트로 이루어져 있다. 더 적은 파라미터로 일반적으로 더 좋은 성능을 갖는 모델이다. LSTM과 GRU 모두 RNN을 기반으로 한 모델이다. 현 시점에서는 RNN을 Transformer로 교체해 나가고 있는 추세이다.  

##### Transformer  

1.  Sequential Data Problems  
    자연어 처리의 도메인에 있어서는 위와 같은 문제 뿐만 아니라 예측하기 다양한 문제가 있다. 단어를 도치하거나, 생략하거나, 문장 구조를 완벽히 재현하지 않는 등의 문제가 있다. 이런 데이터 속에서도 좋은 성능을 내기 위해 고안된 것이 Transformer이다.  
    현 시점에서는 NLP뿐만 아니라 Computer Vision의 도메인에서도 Transformer가 좋은 성능을 내고 있다.

2.  Transformer Core Idea  
    Transformer는 RNN과 달리 재귀적인 구조를 갖지 않는다. 모든 입력 데이터에 대해 한번에 Encoding을 수행한다. 이 Encoding 시에 Self Attention 기법을 사용하여 단어 간의 관계를 효율적으로 계산하고 있다.  
    Transformer는 우선 크게 Encoders와 Decoders로 구분되며, 내부에 각각 6층개의 Network로 구성되어 있다. 하나의 Encoder는 Self-Attention과 Feed Forward Neural Network로 구성되어 있고, 여기의 Self-Attention이 핵심적인 아이디어이다.

3.  Self Attention  
    Transformer의 Encoder는 Embeding 된 입력값을 동시에 Encoding한다. 입력값 $x_i$에 대해서 Encoding을 수행할 때 자신을 포함한 모든 $n$ 입력값과의 관계를 계산하고, 이를 기반으로 $z_i$로 Encoding한다.  
    입력 데이터인 벡터 $\mathbf{x}$가 들어왔을 때 각 원소인 $x_i$ 대해 3개의 Vector를 생성한다. 이는 Query, Key, Value Vector로 불리며 각각의 역할을 수행한다.  
    Encoding 하고자 하는 $x_i$의 Query Vector를 모든 입력 $(x_1,...,x_n)$의 Key Vector와 내적하여 각각의 Score을 산출한다. 이 Score의 값을 줄이기 위해 Normalize하고, 이 값을 Softmax에 통과시킨다. 이를 통해 $x_i$에 대한 $\mathbf{x}$의 관계를 구할 수 있다. 이렇게 구해진 Softmax 값과 $x_i$의 Value를 곱하여 $z_i$로 임베딩한다.  
    위와 같은 절차를 거친 $z_i$는 같이 입력된 모든 데이터와의 관계를 포함한 정보가 된다. 이와 같은 절차는 행렬을 통해 간단히 계산될 수 있다.

4.  Multi Headed Attention  
    위와 같은 절차를 n개의 Head를 가진 Encoder로 동시에 수행할 수 있다. 이렇게 하면, 출력의 차원이 n_head만큼 곱해져서 커지게 되는데, 이를 다시 원래의 Embeding 차원으로 줄여 다음 Encoder에 전달한다.  
    n_head x d_feat * d_feat 형태의 Matrix를 곱하는 방법이 있으나, 구현 상에서는 먼저 차원을 자른 후에 붙이는 방법을 사용했다.

5.  After Self Attention  
    $z_i$로 Encoding 된 벡터는 위치정보 값을 갖지 않기 때문에, Positional Encoding 과정을 거치며 정해진 특정 값이 더해지고, 이후 Feed Forward 과정을 거친다. Encoder에서 생성된 Key와 Value는 Decoder에 전달되고, Decoder는 Query에 따라 특정 값을 생성해내는 역할을 수행한다.