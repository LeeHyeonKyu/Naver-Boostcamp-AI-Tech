## 강의 내용 정리

-   **Data Visualization**
-   **Statistics**

##### Data Visualization

1.  matplotlib  
    `matplotlib`은 Python의 대표적인 시각화 도구이다. R의 시각화 도구를 차용하여 많은 graph와 tool을 제공하며, `pandas`와 호환되는 강력한 라이브러리이다.  
    matplotlib을 사용할 때에는 `pyplot`객체를 사용하며, 이 객체 위에 그래프를 쌓는 개념으로 작동한다. 이 객체 위에 `figure`, `axes`, `plot`등을 쌓아 올린 후, `show`메서드를 통해 출력하는 형식이다. 예시는 다음과 같다.  
    ```python
    # matplotlib example
    import matplotlib.pyplot as plt

    fig = plt.figure()
    ax_1 = fig.add_subplot(1,2,1)
    ax_2 = fig.add_subplot(1,2,1)

    ax_1.plot(X_1, Y_1)
    ax_2.plot(X_2, Y_2)
    plt.show()
    ```

2.  plot's parameters  
    `color` 혹은 `c`는 표시되는 graph의 색상을 지정할 수 있다. `'r'`과 같은 문자열 혹은 rgb값을 통해 지정이 가능하다.  
    `linestyle` 혹은 `ls`는 graph가 line인 경우 모양을 지정할 수 있다.
    `label`을 통해 각 graph가 무엇을 나타내는지 지정할 수 있다.

3.  plt's members  
    `title`을 통해 해당 graph의 이름을 표시해줄 수 있다.
    `legend`를 통해 `label`등을 표시할 수 있다.
    `xlim`과 `ylim`을 통해 표시되는 graph의 범위를 지정할 수 있다.
    `plot`은 기본적으로 lineplot의 형태이나 `scatter`, `bar`, `hist`, `boxplot`등 다양한 graph를 시각화 할 수 있다.

4.  seaborn  
    `seaborn`은 `matplotlib`의 wrapper 형태의 library이다. matplotlib의 기능을 더 간편하게 사용할 수 있는 장점이 있다. 간단한 코드로도 더 유려한 모양의 graph를 만들어 낼 수 있다.

5.  basic plots  
    `lineplot`, `scatterplot`, `countplot`, `barplot`, `distplot`, `violinplot` 등 매우 많은 형태의 graph를 제공한다. 각각의 plot은 필요로 하는 parameter가 다르기 때문에, 필요 시 document나 seaborn 홈페이지의 example을 참고하는 것이 좋다.

6.  multiple plots  
    한 개 이상의 graph를 한번에 그릴 수 있는 plot이다. `replot`, `catplot`, `FacetGrid`, `pairplot`, `lmplot` 등 많은 plot을 제공한다.

##### Statistics

1.  통계학과 기계학습  
    통계적 모델링은 적절한 가정 위에서 확률분포를 추정하는 것이 목표이다. 이는 기계학습이 추구하는 목표와 동일하다. 유한한 갯수의 데이터를 통해서 확률분포를 추정하고 근사하여 앞으로에 대한 예측과 의사결정 등에 활용할 수 있다.  
    이러한 모델을 만드는 것은 데이터의 확률분포를 정확히 맞춘다기 보다는 추정방법의 불확실성을 고려한 상태에서 위험을 최소화 하는 방향으로 진행된다. 즉, 틀릴 확률을 낮추고자 하는 것이다.
    특정 확률분포를 가정하고 추정하는 방법을 **모수적 방법론**이라 하며, 특정 확률분포를 가정하지 않는 경우 **비모수적 방법론**이라고 한다.  

2.  모수적 방법론  
    모수란 특정 확률분포를 정의하는 고정된 상수이다. 각 확률분포는 이 모수에 의해 특징지어지며, 이 모수가 바로 데이터의 특징이 된다.  
    기계학습이나 통계적 모델링은 데이터가 어떤 확률분포를 따르는지 가정하고, 그 이후에 모수를 찾아나가는 과정으로 이어진다. 이때 확률분포를 가정하는 방법은 데이터를 생성하는 원리를 고려하여 결정되어야 한다.  

3.  모수적 방법론 중 정규분포의 예시 I  
    주어진 데이터 $X$가 정규분포를 따른다고 가정했을 때, 정규분포의 모수인 **평균**($\mu$)과 **분산**($\sigma^2$)을 찾게 된다.  
    주어진 데이터로부터 N개의 데이터를 뽑아 표본평균인 $\bar{X}$를 구할 수 있는데, 이런 통계량이 구해지는 확률분포를 **표집분포**(sampling distribution)라 한다. 특히 표본평균의 표집분포는 N이 커질수록 **중심극한의 정리**에 따라 정규분포를 따르게 되어있다.

4.  최대가능도 추정법  
    우리에게 주어진 데이터가 어떤 모수를 가진 확률분포를 따르는지 찾기 위해서 **최대가능도 추정법**(maximum likelihood estimation)을 사용할 수 있다. 식으로 표현하면 다음과 같다. 여기서 $\theta$는 모수를 의미하고, 정규분포에서는 $\mu$와 $\sigma$이다.  
    $\hat{\theta}_{MLE}=argmax_{\theta}L(\theta;\mathbf{x})=argmax_{\theta}P(\mathbf{x}|\theta)$  
    이는 모수 $\theta$를 따르는 어떤 분포가 우리에게 주어진 $\mathbf{x}$를 관측할 가능성을 의미하고, 이 가능성이 높다는 의미는 반대로 우리에게 주어진 데이터가 해당 분포를 따를 가능성이 높다는 의미이다.  
    실제로 계산을 수행할 때에는 `Log`를 씌운 **로그 가능도 함수**를 사용하는데, 이는 아래와 같은 장점이 있기 때문이다.  
    *   데이터가 많을 경우, 컴퓨터로 가능도 함수의 수식 계산은 정확도의 문제가 생긴다.
    *   위 상황에서, 로그를 씌울 경우 덧셈 연산이 되므로 컴퓨터에서 연산이 가능하다.
    *   경사하강법을 가능도를 최적화 할 때 미분을 수행하는데, 연산량을 획기적으로 줄일 수 있다. (가능도 함수는 $O(n^2)$이나, 로그가능도 함수는 $O(n)$임.)

5.  모수적 방법론 중 정규분포의 예시 III  
    정규분포를 따른다고 가정한 데이터의 가능도 함수는 다음과 같다. $\hat{\theta}_{MLE}=argmax_{\mu,\sigma^2}P(\mathbf{X}|\mu,\sigma^2)$  
    위의 식에 Log를 씌은 로그 가능도 함수는 다음과 같이 전개된다. $LogL(\theta;\mathbf{X})=-\frac{n}{2}log2\pi\sigma^2-\sum_{i=1}^{n}{\frac{|x_i-\mu|^2}{2\sigma^2}}$  
    해당 식에서 최대값을 만족하는 $\mu$와 $\sigma^2$를 찾는 방법은 편미분값이 0이 되는 지점을 찾는 것이다. 각 모수로 편미분한 값은 다음과 같다.  
    $\frac{\partial{logL}}{\partial{\mu}}=-\sum_{i=1}^{n}{\frac{x_i-\mu}{\sigma^2}}=0$, $\frac{\partial{logL}}{\partial\sigma}=-\frac{n}{\sigma}+\frac{1}{\sigma^3}\sum_{i=1}^{n}{|x_i-\mu|^2}=0$  
    이를 통해 모수를 추정하면 다음과 같아진다. $\hat{\mu}_{MLE}=\frac{1}{n}\sum_{i=1}^{n}{x_i}$, $\hat{\sigma^2}=\frac{1}{n}\sum_{i=1}^{n}({x_i-\mu})^2$

6.  딥러닝에서 최대가능도 추정법  
    최대가능도 추정법을 통해 기계학습 모델을 학습시킬 수 있다. 분류 문제의 경우, $\mathbf{y}$는 **카테고리 분포**로 부터 생성되었다고 가정할 수 있고, 카테고리 분포의 모수를 모델링 할 수 있다. 정답레이블을 관찰데이터로 이용해 확률분포인 소프트맥스 벡터의 로그가능도를 최적화 할 수 있고, 이를 통해 가중치($\theta$)를 최적화 할 수 있다. 수식은 다음과 같다. $\hat{\theta}_{MLE}=argmax_{\theta}\frac{1}{n}\sum_{i=1}^{n}\sum_{k=1}^{K}{y_i,k}log(MLP_{\theta}(x_i)k)$  

7.  확률분포의 거리  
    기계학습에서 사용되는 손실함수는 모델이 학습하는 확률분포와 데이터 사이에서 관찰되는 확률분포의 거리를 통해 유도한다. 데이터 공간에 두개의 확률분포 $P(x)$, $Q(x)$가 있을 경우 **쿨백-라이블러 발산** 함수 등으로 두 확률분포의 거리를 계산할 수 있다.  
    분류 문제에서 정답 레이블을 $P$, 모델 예측을 $Q$라고 가정하게 된다면, 최대가능도 추정법은 쿨백-라이블러 발산을 최소화 하는 것과 동일한 의미를 갖는다.

---

## 개인 학습

-   **삼각함수**
-   **확률과 통계 기초**

##### 삼각함수
*  삼각비, 일반각, 호도법, 삼각함수와 같은 기초 개념부터 복습했다. 
*  $cos$법칙 등을 재정립했고, 벡터의 내적과 어떻게 상호작용하는지 원리를 학습했다.

##### 확률과 통계 기초
*  이산, 연속확률분포, 모집단과 표본집단, 모수 등 기초 개념부터 복습했다.
*  최대가능도 추정법의 원리와 사용 예시 등을 학습했다.