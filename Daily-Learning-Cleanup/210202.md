## 강의 내용 정리

-   **Optimization**
-   **CNN Tutorial**

##### Optimization

1.  최적화와 관련된 용어 정리  
    최적화에 대해 논의하기 전에 정리해야하는 개념과 용어는 아래와 같다.  
    *   Generalization : Train에 사용하지 않은 Data에 대해 Error가 적도록 하는 것.
    *   Under / Over Fitting : Train Data에 과소 혹은 과대적합되어 일반화 성능이 떨어지는 것.
    *   Cross Validation : Test Set은 어떤 형태로든 Train에 영향을 주어선 안되기 때문에, Train Data의 일부를 통해 일반화 성능을 확인하는 것.
    *   Bias-Variance Trade off : Total Error는 3개의 요소로 이루어져 있으며, Epsilon을 제외한 Bias와 Variance는 상호 Trade off 관계이다. 이는 모델의 복잡도에 따라 조정되며, 모델이 적절한 복잡도를 가졌을 때, Total Error를 줄일 수 있다.
    *   Bootstrapping : 주어진 Dataset 중 일부를 Subsampling하여 Sub Dataset을 만드는 과정을 의미한다.
    *   Bagging and Boosting : Bootstrapping으로 추출된 여러 Sub Dataset을 통해 구조는 같지만 가중치가 다른 모델을 만들어 낼 수 있다. 여러 모델이 관계를 갖지 않고 학습해 Ansemble하는 것을 Bagging이라 하고, 순차적으로 관계를 가지며 학습해 Ansemble하는 것을 Boosting이라고 한다.

2.  Gradient Descent Methods  
    Gradient Descent는 최적화의 가장 기본적인 아이디어이다. 이는 Loss Function의 값을 줄이는 것을 목적으로 작동하며, 이 때 사용되는 Data의 크기(Batch)에 따라 종류를 구분할 수 있다.  
    *   (Full Batch) Gradient Descent : 일반적인 경사하강법으로, 매 Epoch마다 모든 데이터를 통해 Gradient를 계산한다.  
    *   Stochastic Gradient Descent : 확률적 경사하강법으로, 확률적으로 한 개의 Sample Data만을 통해 Gradient를 계산한다. 이를 통해 연산의 속도를 줄일 수 있고,Loss Function이 고정되지 않아 Local Minimize Point를 탈출 할 수 있다.
    *   Mini Batch Gradient Descent : 위 두 방법을 합친 것으로, 매 Epoch마다 정해진 Size만큼의 Data를 통해 Gradient를 계산한다. SGD의 장점과, Flat한 Minimize Point를 찾을 수 있다.

3.  Optimizer  
    위와 같이 Loss Function에 따라 얻어진 Gradient는 Model의 Parameter를 최적화에 사용된다. 이때 최적화 하는 방법에 따라 큰 차이가 생길 수 있다. 대표적인 종류는 다음과 같다.  
    *   SGD
    *   Momentum
    *   Nesterov Accelerated Gradient
    *   Adagrad
    *   Adadelta
    *   RMSprop
    *   Adam

4.  SGD  
    Stochastic Gradient Descent로 구해진 Gradient에 Learning Rate를 곱한 값만을 사용해 Parameter를 최적화하는 방법이다. $W_t$는 $t$번째 Epoch의 가중치를 의미하며, $g_t$는 $t$번째 Gradient 값을 의미한다. $\eta$는 Learning Rate를 뜻한다.  
    $$W_{t+1} \leftarrow W_t - \eta{g_t}$$  

5.  Momentum  
    Momentum은 각 Paramter를 최적화 할 때, 이전의 최적화 값을 고려하는 방법이다. 같은 방향으로 학습하는 경우 더 큰 값으로 학습되고, 반대의 경우 더 작은 값으로 학습된다. 이를 위해 $a$ Term을 추가했고, 이 안에 각 파라미터의 Gradient 값을 누적 저장한다.
    $$a_{t+1} \leftarrow \beta{a_t} + g_t \\ W_{t+1} \leftarrow W_t - \eta{a_{t+1}}$$

6.  Nesterov Accelerate Gradient  
    NAG는 Momentum과 동일하게 이전 Gadient 값을 누적하나, Momentum이 수렴하지 못하는 문제를 해결하고자 Term을 추가했다. Lookahead Grad Term은 실제 그 방향으로 Parameter를 업데이트 한 후 Gradient를 구하는 것으로, 진자운동과 같은 형태로 수렴하지 못하는 문제를 해결할 수 있다.  
    $$a_{t+1} \leftarrow \beta{a_t} + \nabla{L(W_t - \eta\beta{a_t})} \\ W_{t+1} \leftarrow W_t - \eta{a_{t+1}}$$

7.  Adagrad  
    Adagrad는 Parameter의 변화량을 추적하여, 값이 많이 변한 Parameter는 더 작게 이동시키고, 반대의 경우에는 더 크게 이동시킨다. Parameter가 많이 변화 한 경우에는 최적값 근처에 있다고 판단하고, 반대의 경우에는 멀리 있다고 판단하는 것이다. 이를 위해 변화값을 측정할 $G_t$ Term을 추가시키고, 해당 값이 0일 때를 대비하여 $\epsilon$을 추가한다.  
    $$W_{t+1} \leftarrow W_t - \frac{\eta}{\sqrt{G_t + \epsilon}}g_t$$

8.  Adadelta  
    Adadelta는 Adagrad의 $G_t$ Term이 무한히 커져서, 나중에는 학습이 불가능해지는 것을 막기 위해 고안되었다. 분자에 $H_t$ Term을 추가하여, Epoch의 Time Step이 커져도 최적화를 수행할 수 있도록 했다. 이때, Parameter의 이전 모든 Grad를 갖는 대신, 해당 값을 근사한 지수이동평균 값($\gamma$)으로 갖고있게 된다.  
    $$G_t = \gamma G_{t-1} + (1-\gamma)g_t^2 \\ H_t = \gamma H_{t-1} + (1-\gamma)(\Delta W_t)^2 \\ W_{t+1} = W_t - \frac{\sqrt{H_{t-1} + \epsilon}}{\sqrt{G_t + \epsilon}}g_t$$

9.  RMSprop  
    Adadelta와 동일하게, Adagrad의 문제를 해결하고자 했다. Adadelta의 큰 특징 중 하나는 Learning Rate인 $\eta$가 없다는 것인데, $H_t$ Term 대신 이를 사용한 방법이다. Learning Rate을 직접 지정할 수 있어 실제로 Adadelta 보다 더 범용적으로 사용되었다.  
    $$G_t = \gamma G_{t-1} + (1-\gamma)g_t^2 \\ W_{t+1} = W_t - \frac{\eta}{\sqrt{G_t + \epsilon}}g_t$$

10. Adam  
    현재 가장 널리 쓰이고, 좋은 성능을 보이는 Optimizer이다. Adaptive Moment Estimation의 약자로, 지수이동평균의 개념과 Momentum의 방법을 동시에 활용하는 Optimizer이다.  
    $$m_t = \beta_1 m_{t-1} + (1-\beta_1)g_t \\ v_t = \beta_2 v_{t-1} + (1-\beta_2)g_t^2 \\ W_{t+1} = W_t - \frac{\eta}{\sqrt{v_t+\epsilon}} \frac{\sqrt{1-\beta_2^t}}{1-\beta_1^t} m_t$$

11. Regularization  
    모델의 일반화 성능을 높이기 위해 여러 규제를 사용할 수 있다. 이 방법들은  모델의 구조와 같은 개념이 아니라 성능을 올리기 위한 Optional한 방법들이다.  
    *   Early Stopping : Overfitting을 막기 위해 학습 시 Validation Data에 대한 Error를 확인하고, 이 값이 커질때 학습을 종료하는 방법이다.
    *   Parameter Norm Penalty : Loss Function에 Norm Penalty Term을 추가하여, 각 Parameter가 큰 값을 갖지 않도록 하는 방법이다. 이를 통해 모델의 함수 공간 그래프가 부드러워지는 효과를 얻을 수 있다.
    *   Data Augmentation : Train Data를 늘리는 방법의 일환으로 Label이 바뀌지 않는 한에서 Data에 변형을 가하는 방법이다. 
    *   Noise Robustness : Train Data와 Parameter에 Noise를 일부러 추가하여, Noise가 있어도 좋은 성능을 갖는 강건한 모델을 만드는 방법이다.
    *   Label Smoothing : Train Data를 섞어서 Label을 One-hot의 형태가 아니라, 확률의 값을 갖도록 변경하고 학습시키는 방법이다. 이를 통해 모델은 유연한 결정경계를 가질 수 있다.
    *   Dropout : Train 시 일부의 Node를 0으로 만들어 각각의 Parameter가 강건하도록 만드는 학습 방법이다.
    *   Batch Normalization : Layer 사이에서 출력된 Data를 정규화 한 후 다음 Layer에 전달하는 방식이다.

##### CNN Tutorial

1.  CNN  
    CNN은 Fully Connected Layer Model과는 다르게, **Convolution**연산을 사용하는 Neural Network이다.  
    입력 Data의 벡터$\mathbf{x}$를 선형변환하여 벡터 $\mathbf{o}$로 만든다고 했을 때, Fully Connected Layer Model은 각 $\mathbf{o_i}$에 대응하는 $\mathbf{w}_i$를 가져야 했다. 즉, 가중치 행렬인 $\mathbf{W}$가 무거워지고, 많은 Parameter를 가져야 했던 것이다.  
    CNN은 위와 다르게 **Kernel**을 통한 선형변환 연산을 수행한다. 일정한 크기의 Kernel이 이동하며 해당 Kernel Size만큼의 Data와 연산을 하여 선형변환을 하는 개념이다. 즉, Kernel Size만큼의 Parameter만으로 연산을 하는 것으로 Parameter의 수를 줄일 수 있는 것이다.

2.  Convolution 연산의 의미  
    Convolution 연산은 입력된 신호를 Kernel을 이용해 국소적으로 증폭 또는 감소시켜서 정보를 추출하거나 필터링 하는 것이다. 정의역이 연속 공간인지 이산 공간인지에 따라 수식은 변하지만, 기본적인 개념은 동일하다.  
    $$[f*g](x)=\int_{\mathbb{R}^d}{f(z)g(x+z)dz}$$  
    Convolution 연산은 차원에 국한되지 않으며, 여러 차원에 대해 연산을 할 수 있다. 음성이나 텍스트는 1차원 형태의 데이터이며, 영상은 2차원 혹은 3차원의 형태를 가진다.  
    Convolution 연산은 공간의 의미를 내포할 수 있기 때문에, 영상이나 이미지에 많이 활용되고 있다. 커널에 따라 특정 효과를 얻거나, 정보 등을 추출할 수 있다. ([Link](https://setosa.io/ev/image-kernels/))

3.  2D Convolution  
    위의 개념과 마찬가지로 입력 벡터 상에서 Kernel이 움직여가며 선형변환과 합성함수가 적용되는 구조이다. Kernel의 위치에 해당하는 입력 벡터와의 Element-wise Operation을 수행하며, 해당 값의 합으로 새로운 출력을 만든다.  
    Stride가 1이라고 했을 때, 입력 벡터와 출력 벡터의 크기는 다음과 같다. $Output = Input - Kernel Size + 1$  
    색상이 있는 Image의 경우, 해당 Data는 3차원 Tensor이다. 2D 형태의 데이터에 RGB와 같은 채널이 있는 형태이기 때문이다. 이와 같은 경우, Kernel역시 입력 데이터의 채널과 동일한 채널을 가져야 한다.  
    입력 데이터와 같은 채널을 가진 Kernel을 통과하면 1개짜리 채널을 가진 Output이 생기는데, Output의 채널을 n개로 늘리고 싶은 경우에는 동일한 Shape의 Kernel을 n개 사용하면 되겠다.  

4.  Backpropagation of CNN 
    Convolution 연산을 통해 출력된 Output으로부터 역전파를 하게 되면, 마찬가지로 Convolution 연산이 일어난다.  
    $$[f * g'](x)$$  
    Kernel 내의 각 Parameter는 역전파로 전해진 $\delta$와 적용했던 모든 입력값 $x$의 곱의 합이 해당 Parameter의 Grad가 된다.  
    $$\frac{\partial{L}}{\partial{w_i}}=\sum_{j}{\delta_jx_{i+j-1}}$$