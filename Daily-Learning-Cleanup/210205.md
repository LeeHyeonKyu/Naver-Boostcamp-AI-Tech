## 강의 내용 정리

-   **Generative Model I**
-   **Generative Model II**

##### Generative Model I  

1.  Generative Model이란?  
    Generative Model은 High Level의 Data를 출력하는 Model이다. Image나 Text와 같은 형태의 Data를 생성하는 Model로 이해할 수 있다. ([Link](deepgenerativemodels.github.io))  
    Generative Model은 단순히 생성만 할 수 있는 것은 아니다. 주어진 Data로부터 새로운 Data를 만드는 **Sampling**, Data의 특징점을 파악하는 **Feature Learning**, Data와 학습된 확률분포를 비교하는 **Anomaly Detection** 등을 수행할 수 있다.  
    특히, Anomaly Detection은 확률 값을 통해서 이루어진다. 이는 분류와 같은 기능을 수행할 수 있는 모델이라는 의미이며, 이에 따라 **Explicit Model**과 **Implicit Modle**로 구분할 수 있다.  

2.  이산확률분포와 Parameter    
    Binary Class인 경우에는 Bernoulli Distribution을 따르며, 이는 1개의 Parameter만으로도 표현할 수 있다. $P(X)$와 $1-P(X)$로 표현할 수 있기 때문이다.  
    예를 들어, 28x28 형태의 Binary Image를 표현하고자 한다. 가능한 모든 경우의 이미지는 $2^{784}$개이며, 필요한 총 Parameter의 갯수는 $2^{784}-1$개이다.  
    N개의 Class를 갖는 경우에는 Categorical Distribution을 따르며, 이는 N-1개의 Parameter로 표현할 수 있다.  
    예를 들어, RGB의 색상을 갖는 1개의 Pixel에 대해 표현하고자 한다. Pixel은 각각의 채널에서 256개의 값을 가질 수 있으므로, 총 $256^3$개의 값을 가질 수 있다. 이를 표현하기 위해서는 한 Pixel마다 $255^3$개의 Parameter가 필요하다. 만약 위와 같이 784개의 Pixel이 있는 경우에는 총 $784*255^3$개의 Parameter가 필요하다.

3.  Structure Through Independence  
    모든 Pixel의 각각 독립적이라는 가정을 한다면, Parameter의 갯수를 크게 줄일 수 있다. 물론 이와 같은 가정을 통해 생성된 Image는 White Noise에 불과할 것이다.  
    $P(x_1,x_2...,x_n)=P(x_1)P(x_2)...P(x_n)$으로 표현할 수 있다. 784개의 Pixel이 있다고 하면, 가정과 무관하게 마찬가지로 $2^{784}$개의 이미지가 생성될 수 있다. 단, 이를 모델링할 때 필요한 Parameter는 784개에 불과하다.

4.  Conditional Independence  
    Markov Assumption을 통해 Pixel간의 관계를 간단하게 고려해보겠다. 각 Pixel은 바로 이전의 Pixel과만 연관성이 있다고 가정한다면, 아래와 같이 정리될 수 있다.  
    $P(x_1,x_2...,x_n)=P(x_1|x_2)P(x_2|x_3)...P(x_n|x_{n-1})$ 이를 통해 필요한 Parameter의 수를 계산하면, $2n-1$개가 필요한 것을 알 수 있다.  

5.  Fully Dependent Model  
    모든 Pixel이 모두 다른 Pixel과 관계를 갖는다고 할 경우, 다음과 같이 정리 될 수 있다.
    $p(x_1,…,x_n) =p(x_1)p(x_2|x_1)p(x_3|x_1,x_2)⋯p(x_n|x_1,⋯,x_{n−1})$ 이는 위에서 계산한 것과 동일하게, 총 $2^n-1$개의 Parameter로 표현할 수 있다.

6.  Auto Regressive Model  
    Fully Dependent Model은 Parameter가 너무 많기 때문에, 적절한 Assumption으로 Parameter를 줄이는 적합한 모델을 찾고자 한다. Data가 이전 N개의 Data와 관계를 갖는가를 설정한 모델을 AR-n Model이라고 칭한다.  
    이런 Dependency를 결정한다는 것은 즉 Conditional Independency를 결정하는 것이고, 이를 통해 Model의 Structure와 필요한 Parameter의 개수가 결정된다.  

7.  Neural Autoregressive Density Estimator  
    NADE는 Fully Dependent Model이다. $i$번째 입력에 대한 확률분포를 만들 때, $i-1$까지의 모든 입력을 계산하는 형태이다. 예를 들어, $x_3$에 대한 출력은 $x_0,...,x_2$까지의 입력에 해당하는 $w_0,...,w_2$가 필요하다. 즉, 뒤에 있는 데이터일 수록 필요한 Parameter가 많아진다는 특징을 가진다.  
    NADE는 Explicit Model이다. 위에서 보았듯, 입력에 대한 확률 값을 계산하기 때문이다. 각 입력에 대한 확률분포를 만들기 때문에, 모든 확률을 곱하여 확률값을 구할 수 있다.  

8.  Pixel RNN  
    Pixel RNN은 RNN구조를 이용해 Image를 생성하고자 하는 Model이다. AR Model의 형태로 이전 모든 Pixel에 대해 Dependent하며, 채널은 각각 RGB 순으로 Dependent하도록 구성한 구조이다.  
    모든 AR Model이 그렇듯 Ordering에 따라 성능이 다를 수 있는데, Diagonal BiLSTM은 위에 소개한 바와 같고, Row LSTM 구조는 상단의 Pixel에만 Dependent한 구조를 갖는다.

##### Generative Model II  

1.  Variational Auto Encoder  
    Auto Encoder 자체는 사실 Generative Model이 아니다. Auto Encoder 자체의 목적은 Data가 생성된 Distribution을 찾는 것이다. 변이추론(Variational Inference)을 통해 Data의 사후확률분포(Posterior Distribution)을 찾고자 하는 것이다. 이를 기반으로 뒷 단에 다른 Decoder를 활용하는 Model이 Generative Model이다. ([Link](https://www.youtube.com/watch?v=YxtzQbe2UaE&ab_channel=10mindeeplearning))  

2.  Variational Distribution  
    Auto Encoder가 찾고자 하는 사후확률은 직접적으로 구할 수 없기 때문에, 이를 근사하는 방법으로 찾게된다. 즉, True Posterior Distribution과 KL Divergence가 최소화되는  Variational Distribution을 Modeling 하는 것이다.  
    근사의 방법으로는 직접적인 비교가 불가능 하기 때문에, ELBO Object를 도입하여 계산한다. 이 ELBO Term을 최대화 하는 것이 KL Divergence를 최소화 하는 것과 동일하기 때문이다.  

3.  ELBO  
    ELBO는 다시 2개의 Term으로 나눌 수 있다. **Reconstruction Term**과 **Prior Fitting Term**으로 구분할 수 있다.  
    Reconstruction Term은 Decoder를 통해 출력된 값과 실제 Data와의 차이를 직접 비교하는 Term으로 Loss를 통해 계산된다.  
    Prior Fitting Term은 Input Data가 Latent Distribution과 얼마나 유사한지를 비교하는 Term이다. Distribution의 비교는 모든 점에서 미분이 가능해야하므로, 다양한 확률분포를 가정할 수 없다. 그렇기 때문에 통상 Gaussian Distribution을 사용한다.

4.  Adversarial Auto Encoder  
    이 모델은 Auto Encoder에서 Prior Fitting Term을 GAN Object로 바꾼 것에 불과하다. 하지만, 이를 통해서 다양한 확률분포 상의 비교가 가능해졌다. 일반화 성능도 기존의 Auto Encoder에 비해 좋은 것으로 알려져 있다.

5.  Generative Adversarial Network  
    GAN은 **Generator**뿐만 아니라 **Discriminator**를 같이 학습시켜 우수한 생성모델을 만드는 기법이다.  
    Auto Encoder는 입력데이터 $x$를 Latent Distribution의 $P(z)$로 Encoding 한 후, Decoding하여 실제 $x$와 같이 만드는 개념이었다.  
    GAN은 실제 데이터와 생성 데이터를 구분하는 Discriminator와 Discriminator가 구분하지 못하도록 데이터를 생성하는 Generator의 Two Player Game형태로 학습한다.