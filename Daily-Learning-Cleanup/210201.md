## 강의 내용 정리

-   **베이즈 통계학 기초**
-   **DL Historical Review**
-   **PyTorch 기초**
-   **Neural Networks**
-   **DL Dataset Handling**

##### 베이즈 통계학 기초

1.  조건부 확률이란?  
    조건부 확률은 $P(A|B)$과 같이 표기한다. 이 식의 의미는 특정 사건 B가 일어난 상황에서 특정 사건 A가 발생할 확률을 의미한다.  
    사건 A와 B가 동시에 일어날 확률에서, 사건 B가 먼저 주어졌을 확률을 계산하는 것으로 수식은 다음과 같다. $P(A|B) = \frac{P(A \cap B)}{P(B)}$  
    이 식을 응용하면 다음과 같아지며, 이를 **베이즈 정리**라 칭한다.
    $$P(B|A)=\frac{P(A \cap B)}{P(A)}=P(B)\frac{P(A|B)}{P(A)}$$

2.  베이즈 정리의 필요성  
    베이즈 정리는 조건부 확률을 이용하여 정보를 갱신하는 방법이다. A가 조건부로 있을 때의 확률을 반대로 B가 조건부로 있었을 때의 확률로 계산할 수 있다. 이는 선후관계를 바꾸어 확률을 계산할 수 있다는 의미이다.  
    딥러닝에서는 이를 통해 데이터의 갱신과 모수를 추정하는데 사용할 수 있다.

3.  베이즈 정리의 예제 I  
    $$P(\theta|\mathscr{D})=P(\theta)\frac{P(\mathscr{D}|\theta)}{P(\mathscr{D})}$$  
    베이즈 정리를 통해 좌변을 우변과 같이 정리할 수 있다. 각 확률은 다음과 같은 의미를 가진다.  
    $P(\theta|\mathscr{D})$은, **사후확률**로 최종적으로 구하고자 하는 값이다.  
    $P(\theta)$은, **사전확률**로 이미 알려져있는 값이거나 모수에 해당하는 값이며, 주어져있지 않은 경우 추론을 통해 임의의 값을 선정한다.  
    $P(\mathscr{D}|\theta)$은, 사전확률($P(\theta)$)이 주어졌을 때 우리의 관측 데이터가 발견될 확률($\mathscr{D}$)로, **가능도**를 의미한다.
    $P(\mathscr{D})$은, **Ecidence**로, 우리에게 주어진 데이터 전체의 분포를 의미한다.

4.  베이즈 정리의 예제 II  
    어떤 질병의 감염률은 10%로 알려져 있다. ($P(\theta) = 0.1$)  
    해당 질병에 감염되었을 때, 양성으로 검진될 확률은 99%이다. ($P(\mathscr{D}|\theta) = 0.99$)  
    해당 질병에 감염되지 않았으나, 양성으로 검진될 확률은 10%이다. ($P(\mathscr{D}|\neg \theta) = 0.1$)  
    어떤 사람이 양성판정을 받았을 때, 실제 질병에 감염되었을 확률은 얼마인가? ($P(\theta|\mathscr{D})$)  

5.  베이즈 정리의 예제 III  
    단 한가지를 제외한 모든 확률은 구해져 있다. 우리가 구해야 할 것은 Evidence이다.  
    해당 문제에서 데이터의 분포를 알 수는 없으나, 우리는 질병 감염의 여부($P(\theta) or P(\neg\theta)$)를 통해 이를 구할 수 있다.  
    질병의 감염여부는 반드시 해당 질병에 걸렸거나, 걸리지 않았거나 단 두개의 값만 취할 수 있다. 즉, $P(\mathscr{D}) = P(\mathscr{D}|\theta) + P(\mathscr{D}|\neg\theta)$이다.  
    $P(\mathscr{D}) = 0.99*0.1 + 0.1*0.9 = 0.189$가 된다.

6.  베이즈 정리의 예제 IV  
    $P(\theta|\mathscr{D})=P(\theta)\frac{P(\mathscr{D}|\theta)}{P(\mathscr{D})}=0.1*\frac{0.99}{0.189} \approx 0.524$  
    즉, 어떤 사람이 양성판정을 받았을 때, 실제 질병에 감염되었을 확률은 52.4%에 불과하다.  

7.  Confusion Matrix  
    ||$P(\theta)$|$P(\neg\theta)$|
    |:-:|:-:|:-:|
    |$P(\mathscr{D})$|TP|FP|
    |$P(\neg\mathscr{D})$|FN|TN|  
    
    위 표는 모델이 판정한 결과($P(\mathscr{D})$ or $P(\neg\mathscr{D})$)와 실제($P(\theta)$ or $P(\neg\theta)$)를 기준으로 생성된 표이다.  
    위의 물음은 **민감도(Recall)**라 칭하는 $\frac{TP}{TP+FP}$의 값을 구하는 것이다. 이 식에서 FP의 값이 줄어든다면, 민감도는 크게 높아질 것이다.  

8.  베이즈 정리를 통한 정보의 갱신  
    새로운 데이터가 들어왔을 때, 앞서 계산한 사후확률을 사전확률로 사용해 사후확률을 계산할 수 있다.  
    예를 들어, "이미 양성 판정을 받은 사람이 두 번째 검진에도 양성이 나왔을 때, 진짜 질병에 걸렸을 확률은 얼마인가?" 와 같은 문제를 풀 때 사용할 수 있겠다.  
    이처럼 딥러닝에서도, 이미 학습된 데이터외에 추가적인 데이터가 들어오면 베이즈 정리를 통해서 새로운 분포와 모수를 찾아나갈 수 있다.  

9.  조건부 확률과 인과관계  
    조건부 확률은 유용한 통계적 해석을 제공하지만, 이를 인과관계와 동일하게 생각해서는 안된다. 예를 들어, 아이스크림의 판매량과 익사의 비율은 양의 상관관계를 가지나 뚜렷한 인과관계를 갖지는 않는다.  
    조건부 확률만으로 모델을 구성했을 경우, 예측 정확도를 높일수는 있으나 데이터 분포 변화 등에 크게 영향을 받을 수 있다.  
    인과관계를 고려한 모델을 구성했을 경우, 당장의 예측 정확도가 다소 떨어질 수는 있으나 데이터 분포 변화 등에 강건한 모델을 만들 수 있다.  
    즉, 모델을 구성할 때에는 데이터간의 사실적 관계와 도메인 지식 등을 통해 강건한 모델을 만드는 것이 중요하다.  

##### DL Historical Review 

1.  딥러닝의 역사 개요  
    딥러닝은 한 사람이 모든 분야를 다 수행할 수 없고, 많은 사람들이 각자의 분야에서 발전시켜가는 학문이다.  
    이미지라는 한 분야만 하더라도 다음과 같이 많은 세부 분야가 있다.  
    *   Classification
    *   Semantic Sementation
    *   Detection
    *   Pose Estimation
    *   Visual QNA  

    각 세부 분야에서도 더 세부적인 상황과 요구사항이 있고, 이를 발전시켜나가며 많은 논문이 나오고 있다. 논문을 볼 때에는 딥러닝의 키가 되는 요소를 중점적으로 보면, 해당 논문의 요지와 내용을 이해하기 쉽다.  
    딥러닝의 키 요소는 다음과 같다.  
    *   Data : 어떤 데이터를 사용하여 학습을 했는가
    *   Model : 어떤 형태의 모델을 사용했는가
    *   Loss Function : 해결하고자 하는 문제를 어떻게 손실함수로 정의했는가
    *   Optimization : 모델의 파라미터를 어떻게 최적화 했는가

2.  연도별 주요 논문  
    아래의 논문은 딥러닝 혹은 세상의 패러다임을 변화시킨 중요한 논문들이다. ([Link](https://dennybritz.com/blog/deep-learning-most-important-ideas/))  
    1.  2012년 AlexNet
    2.  2013년 DQN
    3.  2014년 Encoder/Decoder, Adam
    4.  2015년 GAN, ResNet
    5.  2017년 Transformer
    6.  2018년 Bert
    7.  2019년 GPT-X
    8.  2020년 Self-Supervised Learning

3.  AlexNet  
    ILSVRC 대회에서 우승을 차지한 모델이다. 최초로 딥러닝 기법을 활용해서 우승을 차지했으며, 이 논문 및 모델의 출현 이후 대회의 모든 방법론이 딥러닝 기법으로 바뀌었다. 즉, 머신러닝에서 딥러닝으로의 패러다임 변화를 이끈 모델이라고 할 수 있다.

4.  DQN  
    Deep Mind사가 Atari라는 게임을 대상으로 만든 강화학습 모델이다. Alphago가 있게 한 논문이라고 할 수 있다.  

5.  Encoder / Decoder  
    자연어처리를 위해 만들어진 모델이다. 어떤 언어를 다른 언어로 표현하고자 한 것임. 단어의 연속을 다른 언어의 단어의 연속으로 의미를 전달하는 모델이다. 이를 통해 기계 번역에 큰 변화가 생겼다.

6.  Adam  
    학습의 방법인 Optimazation에 큰 변화를 가져왔다. 현재는 기본적으로 이 Optimazor를 사용할 정도로 좋은 성능을 내는 방법이다. Optimizor는 딥러닝 모델의 성능을 결정하는 중요한 하이퍼 파라미터이기 때문에, 이 논문은 딥러닝의 큰 발전을 이끌었다.  

7.  GAN  
    Generative Adversarial Network의 줄임말이다. 이미지 생성 및 텍스트 생성에 관련된 논문이다. 딥러닝에서 매우 중요한 Topic이다. 후에 더 자세히 살펴 볼 것이다.  

8.  ResNet  
    Layer를 Deep하게 쌓을 수 있도록 이론적 배경이 된 논문이다. 기존에는 네트워크를 깊게 쌓을 경우 성능이 나빠지는 문제가 있었으나, 이 논문을 통해 일정 이상의 깊이와 좋은 성능을 내는 모델을 만들 수 있게 되었다.  

9.  Transformer  
    Google의 논문이며, 최초에는 자연어처리 분야에서만 좋은 성능을 내는 모델이었다. 허나 점점 더 넓은 분야에 적용되며, 현재는 Computer Vison의 영역에서도 좋은 성능을 내며 많은 모델을 대체하고 있다.  

10. Bert  
    Transformer를 활용하는 모델이며, 자연어처리에 큰 변화를 가져온 모델이다. fine-tuned NLP Model의 시초이다. pre-trained 모델에 내가 풀고자하는 문제의 데이터를 학습시키고 문제를 해결하는 방식을 의미한다.  

11. GPT-X  
    Language모델이며, 약간의 fine tuning을 통해 수많은 어플리케이션이나 서비스를 만들 수 있는 기반이 된다. 매우 많은 파라미터가 있으며, 이미 학습되어 있는 형태이다.

12. Self Supervised Learning  
    한정 된 학습 데이터 외에 label이 없는 unsupervised 데이터를 사용하여 학습하는 개념이다. 이를 학습에 같이 활용하여 모델의 성능을 올리는 방법이다. 이미지를 기계가 이해할 수 있는 좋은 표현을 고려하게 되었고, 이를 통해 많은 성과를 거뒀다.
    이미지에 대한 좋은 기계적 표현을 통해 학습데이터 외의 데이터를 활용해 학습하는 것
    SimCLR
    내가 풀고자하는 문제에 대해 잘 알고있고, 도메인을 아는 경우에 데이터를 만들어내는 트렌드도 있음.

##### PyTorch 기초

1. IDE 환경 구성  
    각종 모듈 생성 등을 위해 colab, google drive, vscode를 연동한다. ([Link](https://www.youtube.com/watch?v=N5WojMutddQ&ab_channel=Fireship))

2.  PyTorch 기초  
    `import torch`로 PyTorch를 import 할 수 있다.  
    torch는 numpy와 동일하게 array를 처리해준다. 거의 1:1 대응되며 각종 기능을 사용할 수 있다. `np.ndarray`는 torch에서 `tensor` 자료형으로 사용된다.  
    `tensor`는 `np.ndarray`와 거의 동일한 기능을 다른 메서드로 구현한다. 예를 들어, `ndarry.shape`은 `tensor.size()`로, `ndarray.reshape()`은 `tensor.view()`로, `ndarray.dot()`은 `tensor.matmul()`등으로 구현되어 있다.  

3.  torch.nn.functional  
    PyTorch는 각종 함수를 `torch.nn.functional` 모듈에 구현해놓았으며, 이는 통상 `F`라는 별칭으로 지정해 사용한다. 예시는 다음과 같다.  
    ``` python
    import torch.nn.functional as F
    F.softmax(tensor, dim=0)
    ```

4.  Autograd  
    PyTorch의 큰 강점 중 하나는 Tensor 객체에 자동 미분 값을 담을 수 있다는 것이다. 이를 통해 역전파와 최적화를 간편하게 구현할 수 있다.  
    ``` python
    import torch
    w = torch.tensor(2.0, requires_grad = True)
    y = w**2
    z = 2*y + 5
    z.backward()
    ```

5.  예제 코드  
    GitHub에 Notebook File로 업로드 해놓았습니다. 자세한 예시는 링크의 **210201_Practice_PyTorch**를 참조해주세요. ([Link](https://github.com/LeeHyeonKyu/Naver-Boostcamp-AI-Tech/tree/main/Daily-Learning-Cleanup))

##### Neural Networks

1.  Neural Networks란?  
    Neural Network는 인간의 뇌의 신호전달을 아이디어로 만들어진 기계학습의 방법론 중 하나이다. 현재 Deep Learning이라 불리는 학습 방법이 이와 같은 형태로 구현되고 있다.  
    Deep Learning은 데이터를 통해 현실 세계의 어떤 현상이나 사건을 만드는 특정 함수를 근사하는 형태로 구현된다. 주어진 데이터를 선형 및 비선형변환하여, 특정 함수에 근사하고 실제와 비슷한 형태의 output을 만들어 낼 수 있다.  

2.  Neural Network 예제  
    PyTorch로 간단한 2층 구조의 MLP를 구현했습니다. 자세한 예시는 링크의 **210201_MLP**를 참조해주세요. ([Link](https://github.com/LeeHyeonKyu/Naver-Boostcamp-AI-Tech/tree/main/Daily-Learning-Cleanup))  

##### DL Dataset Handling

1.  OOP  
    실무는 colab과 같은 remote 환경에서 작업을 하는 경우가 많다. 이 환경에서 terminal을 사용하는데 익숙해져야 하고, 완전히 동작하는 서비스나 모델을 만들어야 한다. 그렇게 하기 위해서는 OOP형태로 구현하는 능력을 길러야한다.  
    한 개의 Notebook File에 모든 코드를 넣어놓게 되면, 다양한 실험이나 유연한 변환이 불가능하고, 병렬적인 처리를 하는데 어려움을 겪을 수 있다. 그렇기 때문에 각각의 기능을 모듈화하여 python script file로 만들고, 이를 import하여 사용할 수 있도록 구성하기를 권장한다.  

2.  Dataset and DataLoader  
    PyTorch에서 데이터를 불러오고 학습에 사용할 때에는 여러 방법을 사용할 수 있다. 대표적으로는 데이터를 불러온 후에 `Epoch`마다 직접 `Batch Size`로 분할하는 방법과, Iterable한 `DataLoader` 객체를 만드는 방법이 있다. 후자에 대한 예시는 다음과 같다.  
    먼저, `torch.utils.data.Dataset`을 상속받는 dataset class를 구현한다. 이 class는 기본적으로 `__init__`, `__len__`, `__getitem__`을 구현한다. 이때, `__getitem__`은 `Tensor`를 반환하도록 한다.  
    이렇게 만들어진 `dataset`을 매개변수로 하는 `DataLoader` 객체를 생성해 학습에 사용할 수 있다. 이 역시 `torch.utils.data.DataLoader`를 상속하여 구현할 수 있다. 대표적인 매개변수는 `dataset`, `batch_size`, `shuffle`가 있다.

3.  Image Data  
    `PIL` Module은 Python에서 이미지를 처리하는 대표적인 라이브러리이다. `PIL.Image.open()`을 통해 File 객체를 불러올 수 있고, 이는 `numpy`등을 통해 `vector`로 형변환 할 수 있다.  
    `torchvision.transforms.ToTensor()(image_obj)`을 통해 이미지 형태의 데이터를 Tensor로 변환할 수도 있고, 반대로 `torchvision.transforms.ToPILImage()(Tensor_obj).convert('RGB')` 등으로 다시 PIL Image 객체로 변환할 수도 있다.