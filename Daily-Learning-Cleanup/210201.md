## 강의 내용 정리

-   **베이즈 통계학 기초**
-   **DL Historical Review**
-   **PyTorch 기초**
-   **Neural Networks**
-   **DL Dataset Handling**

##### 베이즈 통계학 기초

1.  조건부 확률이란?  
    조건부 확률은 $P(A|B)$과 같이 표기한다. 이 식의 의미는 특정 사건 B가 일어난 상황에서 특정 사건 A가 발생할 확률을 의미한다.  
    사건 A와 B가 동시에 일어날 확률에서, 사건 B가 먼저 주어졌을 확률을 계산하는 것으로 수식은 다음과 같다. $P(A|B) = \frac{P(A \cap B)}{P(B)}$  
    이 식을 응용하면 다음과 같아지며, 이를 **베이즈 정리**라 칭한다.
    $$P(B|A)=\frac{P(A \cap B)}{P(A)}=P(B)\frac{P(A|B)}{P(A)}$$

2.  베이즈 정리의 필요성  
    베이즈 정리는 조건부 확률을 이용하여 정보를 갱신하는 방법이다. A가 조건부로 있을 때의 확률을 반대로 B가 조건부로 있었을 때의 확률로 계산할 수 있다. 이는 선후관계를 바꾸어 확률을 계산할 수 있다는 의미이다.  
    딥러닝에서는 이를 통해 데이터의 갱신과 모수를 추정하는데 사용할 수 있다.

3.  베이즈 정리의 예제 I  
    $$P(\theta|\mathscr{D})=P(\theta)\frac{P(\mathscr{D}|\theta)}{P(\mathscr{D})}$$  
    베이즈 정리를 통해 좌변을 우변과 같이 정리할 수 있다. 각 확률은 다음과 같은 의미를 가진다.  
    $P(\theta|\mathscr{D})$은, **사후확률**로 최종적으로 구하고자 하는 값이다.  
    $P(\theta)$은, **사전확률**로 이미 알려져있는 값이거나 모수에 해당하는 값이며, 주어져있지 않은 경우 추론을 통해 임의의 값을 선정한다.  
    $P(\mathscr{D}|\theta)$은, 사전확률($P(\theta)$)이 주어졌을 때 우리의 관측 데이터가 발견될 확률($\mathscr{D}$)로, **가능도**를 의미한다.
    $P(\mathscr{D})$은, **Ecidence**로, 우리에게 주어진 데이터 전체의 분포를 의미한다.

4.  베이즈 정리의 예제 II  
    어떤 질병의 감염률은 10%로 알려져 있다. ($P(\theta) = 0.1$)  
    해당 질병에 감염되었을 때, 양성으로 검진될 확률은 99%이다. ($P(\mathscr{D}|\theta) = 0.99$)  
    해당 질병에 감염되지 않았으나, 양성으로 검진될 확률은 10%이다. ($P(\mathscr{D}|\neg \theta) = 0.1$)  
    어떤 사람이 양성판정을 받았을 때, 실제 질병에 감염되었을 확률은 얼마인가? ($P(\theta|\mathscr{D})$)  

5.  베이즈 정리의 예제 III  
    단 한가지를 제외한 모든 확률은 구해져 있다. 우리가 구해야 할 것은 Evidence이다.  
    해당 문제에서 데이터의 분포를 알 수는 없으나, 우리는 질병 감염의 여부($P(\theta) or P(\neg\theta)$)를 통해 이를 구할 수 있다.  
    질병의 감염여부는 반드시 해당 질병에 걸렸거나, 걸리지 않았거나 단 두개의 값만 취할 수 있다. 즉, $P(\mathscr{D}) = P(\mathscr{D}|\theta) + P(\mathscr{D}|\neg\theta)$이다.  
    $P(\mathscr{D}) = 0.99*0.1 + 0.1*0.9 = 0.189$가 된다.

6.  베이즈 정리의 예제 IV  
    $P(\theta|\mathscr{D})=P(\theta)\frac{P(\mathscr{D}|\theta)}{P(\mathscr{D})}=0.1*\frac{0.99}{0.189} \approx 0.524$  
    즉, 어떤 사람이 양성판정을 받았을 때, 실제 질병에 감염되었을 확률은 52.4%에 불과하다.  

7.  Confusion Matrix  
    ||$P(\theta)$|$P(\neg\theta)$|
    |:-:|:-:|:-:|
    |$P(\mathscr{D})$|TP|FP|
    |$P(\neg\mathscr{D})$|FN|TN|  
    
    위 표는 모델이 판정한 결과($P(\mathscr{D})$ or $P(\neg\mathscr{D})$)와 실제($P(\theta)$ or $P(\neg\theta)$)를 기준으로 생성된 표이다.  
    위의 물음은 **민감도(Recall)**라 칭하는 $\frac{TP}{TP+FP}$의 값을 구하는 것이다. 이 식에서 FP의 값이 줄어든다면, 민감도는 크게 높아질 것이다.  

8.  베이즈 정리를 통한 정보의 갱신  
    새로운 데이터가 들어왔을 때, 앞서 계산한 사후확률을 사전확률로 사용해 사후확률을 계산할 수 있다.  
    예를 들어, "이미 양성 판정을 받은 사람이 두 번째 검진에도 양성이 나왔을 때, 진짜 질병에 걸렸을 확률은 얼마인가?" 와 같은 문제를 풀 때 사용할 수 있겠다.  
    이처럼 딥러닝에서도, 이미 학습된 데이터외에 추가적인 데이터가 들어오면 베이즈 정리를 통해서 새로운 분포와 모수를 찾아나갈 수 있다.  

9.  조건부 확률과 인과관계  
    조건부 확률은 유용한 통계적 해석을 제공하지만, 이를 인과관계와 동일하게 생각해서는 안된다. 예를 들어, 아이스크림의 판매량과 익사의 비율은 양의 상관관계를 가지나 뚜렷한 인과관계를 갖지는 않는다.  
    조건부 확률만으로 모델을 구성했을 경우, 예측 정확도를 높일수는 있으나 데이터 분포 변화 등에 크게 영향을 받을 수 있다.  
    인과관계를 고려한 모델을 구성했을 경우, 당장의 예측 정확도가 다소 떨어질 수는 있으나 데이터 분포 변화 등에 강건한 모델을 만들 수 있다.  
    즉, 모델을 구성할 때에는 데이터간의 사실적 관계와 도메인 지식 등을 통해 강건한 모델을 만드는 것이 중요하다.  

##### DL Historical Review 

1.  딥러닝의 역사 개요  
    딥러닝은 한 사람이 모든 분야를 다 수행할 수 없고, 많은 사람들이 각자의 분야에서 발전시켜가는 학문이다.  
    이미지라는 한 분야만 하더라도 다음과 같이 많은 세부 분야가 있다.  
    *   Classification
    *   Semantic Sementation
    *   Detection
    *   Pose Estimation
    *   Visual QNA  

    각 세부 분야에서도 더 세부적인 상황과 요구사항이 있고, 이를 발전시켜나가며 많은 논문이 나오고 있다. 논문을 볼 때에는 딥러닝의 키가 되는 요소를 중점적으로 보면, 해당 논문의 요지와 내용을 이해하기 쉽다.  
    딥러닝의 키 요소는 다음과 같다.  
    *   Data : 어떤 데이터를 사용하여 학습을 했는가
    *   Model : 어떤 형태의 모델을 사용했는가
    *   Loss Function : 해결하고자 하는 문제를 어떻게 손실함수로 정의했는가
    *   Optimization : 모델의 파라미터를 어떻게 최적화 했는가

2.  연도별 주요 논문  
    아래의 논문은 딥러닝 혹은 세상의 패러다임을 변화시킨 중요한 논문들이다.  
    1.  2012년 AlexNet
    2.  2013년 DQN
    3.  2014년 Encoder/Decoder, Adam
    4.  2015년 GAN, ResNet
    5.  2017년 Transformer
    6.  2018년 Bert
    7.  2019년 GPT-X
    8.  2020년 Self-Supervised Learning

##### PyTorch 기초

1. IDE 환경 구성  
    각종 모듈 생성 등을 위해 colab, google drive, vscode를 연동한다. ([Link](https://www.youtube.com/watch?v=N5WojMutddQ&ab_channel=Fireship))

2.  PyTorch 기초  
    `import torch`로 PyTorch를 import 할 수 있다.  
    torch는 numpy와 동일하게 array를 처리해준다. 거의 1:1 대응되며 각종 기능을 사용할 수 있다. `np.ndarray`는 torch에서 `tensor` 자료형으로 사용된다.  
    `tensor`는 `np.ndarray`와 거의 동일한 기능을 다른 메서드로 구현한다. 예를 들어, `ndarry.shape`은 `tensor.size()`로, `ndarray.reshape()`은 `tensor.view()`로, `ndarray.dot()`은 `tensor.matmul()`등으로 구현되어 있다.  

3.  torch.nn.functional  
    PyTorch는 각종 함수를 `torch.nn.functional` 모듈에 구현해놓았으며, 이는 통상 `F`라는 별칭으로 지정해 사용한다. 예시는 다음과 같다.  
    ``` python
    import torch.nn.functional as F
    F.softmax(tensor, dim=0)
    ```

4.  Autograd  
    PyTorch의 큰 강점 중 하나는 Tensor 객체에 자동 미분 값을 담을 수 있다는 것이다. 이를 통해 역전파와 최적화를 간편하게 구현할 수 있다.  
    ``` python
    import torch
    w = torch.tensor(2.0, requires_grad = True)
    y = w**2
    z = 2*y + 5
    z.backward()
    ```