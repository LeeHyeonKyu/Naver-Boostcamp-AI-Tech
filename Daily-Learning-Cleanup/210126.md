## 강의 내용 정리

-   **경사하강법 I**
-   **경사하강법 II**

##### 경사하강법 I

1.  경사하강법이란?  
    경사하강법(**Gradient Descent**)은 기계학습에서 자주 사용되는 최적화 방법이다. 데이터에 대해 오차를 최소화하는 Parameter를 구할 때 사용되며, 이를 이해하기 위해선 **미분**에 대해서 먼저 알아야 한다.  

2.  미분이란?  
    미분은 영어로 **Differentiation**이다. 말 그대로, 변수의 움직임에 따라 변화하는 함수값을 측정할 때 사용되는 개념이다. 함수 내에서 미분을 사용하면 변수의 특정 값에서의 기울기를 구할 수 있다.  
    $f(x) = x^2 + 2x + 3$의 미분은 $f'(x) = 2x + 2$이다. $x$의 값에 따라 기울기는 양수가 될 수도, 음수가 될 수도 있다.  
    Python에서는 `sympy` Module을 통해 미분을 수행할 수 있다.
    ``` python
    # sympy module example
    import sympy as sym
    from sympy.abc import x
    sym.diff(sym.poly(x**2 + 2*x + 3), x)  # Poly(2𝑥+2,𝑥,𝑑𝑜𝑚𝑎𝑖𝑛=ℤ)
    ```

3.  미분과 경사하강법  
    미분을 통해 구한 값. 즉, 기울기는 변수의 증감에 따른 함수값의 증감을 알려준다. 이 때, 어떤 경우이든 $x$에 $f'(x)$값을 더하면 $f(x)$는 증가하고, 빼면 감소한다. $f'(x)$의 값이 양수이든 음수이든 어떤 경우에도 동일하다.  
    만약 $f(x)$의 값을 증가시시키고 싶다면, $x$에 $f'(x)$를 더하면 된다. $f'(x) = 0$인 지점에 도달하게 되면, **극대값**에 해당하는 $x$를 찾은 것이다. 이를 **경사상승법**이라고 칭한다.  
    우리는 **극소값**에 해당하는 $x$의 값을 찾기를 바라기 때문에, $f'(x)=0$인 지점까지 $x$에서 $f'(x)$를 뺄 것이다. 기울기를 따라 계속 하강하기 때문에 **경사하강법**, **Gradient descent**라고 한다.  
    단, Code 상에서는 `0`의 값 대신 일정 수치에 해당하는 `epsilon`을 지정할 것인데, 이는 컴퓨터 상에서 완전한 `0`의 값을 갖기는 매우 어렵기 때문이다. 경사하강법 알고리즘은 다음과 같이 구현할 수 있다.
    ``` python
    # gradient descent algorithm example
    var = init  # 초기값을 임의로 지정
    grad = gradient(var)  # grad는 var값에서의 기울기 
    while abs(grad) > eps :  # 오차에 해당하는 eps보다 기울기가 큰 경우 반복
        var = var - (lr * grad)  # 해당 값에서 기울기를 빼되, learning rate를 곱하여 계산
        grad = gardient(var)  # 이동한 값에서의 기울기 계산
    ```

4.  경사하강법 예시  
    $x^2 + 2x + 3$ 함수에 경사하강법을 적용해 최소값과 x값을 찾는 예시이다.
    ``` python
    # polynomial and gradient descent example
    import numpy as np
    import sympy as sym
    from sympy.abc import x

    def func(val) :
    fun = sym.poly(x**2 + 2*x + 3)
    return fun.subs(x, val), fun

    def func_gradient(fun, val) :
    _, function = fun(val)
    diff = sym.diff(function, x)
    return diff.subs(x, val), diff

    def gradient_descent(fun, init_point, lr_rate=0.01, epsilon=0.0001) :
    cnt = 0
    val = init_point
    diff, _ = func_gradient(fun, init_point)
    while np.abs(diff) > epsilon :
        val = val - (lr_rate * diff)
        diff, _ = func_gradient(fun, val)
        cnt += 1
    
    print(f'함수:{fun(val)[1]}, 연산횟수:{cnt},\n 최소값:{fun(val)[0]}, x값:{val}')

    gradient_descent(fun=func, init_point=np.random.uniform(-2,2))
    # 함수:Poly(x**2 + 2*x + 3, x, domain='ZZ'), 연산횟수:492, 
    # 최소값:2.00000000241269, x값:-0.999950880817374
    ```

5. 변수가 2개 이상인 벡터의 경우  
    변수가 1개인 경우 위와 같이 통미분을 통해 경사하강법을 쓸 수 있다. 변수의 갯수가 늘어나는 경우에는 이와 같은 방법을 쓸 수 없다. 변수의 수 만큼 차원이 늘어나기 때문에, 공간 상에서의 움직임을 결정해야하기 때문이다.  
    변수가 2개 이상. 즉, 벡터인 경우에는 **편미분**을 통해 **Gradient Vector**를 구해야한다.  
    $f(x,y) = x^2 + 2xy + 3 + cos(x+2y)$일 때, 각 $x$와 $y$에 대한 편미분을 구하면 다음과 같다.  
    $\delta_xf(x,y) = 2x + 2y - sin(x+2y)$, $\delta_xf(x,y) = 2x - sin(x+2y)$  
    Python코드로는 다음과 같이 구현할 수 있다.
    ``` python
    import sympy as sym
    from sympy.abc import x, y

    sym.diff(sym.poly(x**2 + 2*x*y + 3) + sym.cos(x + 2*y), x)  # 2𝑥+2𝑦−sin(𝑥+2𝑦)
    sym.diff(sym.poly(x**2 + 2*x*y + 3) + sym.cos(x + 2*y), y)  # 2𝑥−2sin(𝑥+2𝑦)
    ```
    이 두 값을 요소로 갖는 벡터가 Gradient Vector이며, 공간 상에서 특정 값의 기울기가 된다. 표기는 $\nabla{f} = (\delta_xf, \delta_yf)$와 같다. 이를 일반화하여 표현하면, $x$와 $y$대신 $(x_1, x_2, ..., x_d)$로 표현 된다.  
    $\nabla{f}$를 통해 모든 변수 벡터 $\mathbf{x}=(x_1, x_2, ..., x_d)$를 동시에 업데이트 할 수 있다. 단변수인 경우에는 기울기를 바로 사용할 수 있었으나, 벡터는 크기의 값을 의미하는 `norm`을 구해야한다. 업데이트에 대한 식은 다음과 같다. $\mathbf{x}^{(t+1)} \leftarrow \mathbf{x}^{(t)}-\nabla{f}$  
    Python Code로는 다음과 같다.  
    ``` python
    # multi polynomial and gradient descent example
    var = init  # 초기값을 임의로 지정
    grad = gradient(var)  # grad는 해당 값에서의 gradient vector
    while norm(grad) > eps :  # 오차에 해당하는 eps보다 vector의 norm이 큰 경우 반복
        var = var - (lr * grad)  # 해당 값에서 기울기를 빼되, learning rate를 곱하여 계산
        grad = gardient(var)  # 이동한 값에서의 gradient vector 계산
    ```

6.  다변수 경사하강법 예시  
    $x^2 + 2x + 3$ 함수에 경사하강법을 적용해 최소값과 x값을 찾는 예시이다.
    ``` python
    # multi polynomial and gradient descent example
    import numpy as np
    import sympy as sym
    from sympy.abc import x, y

    def eval_(fun, val) :
        val_x, val_y = val
        fun_eval = fun.subs(x, val_x).subs(y, val_y)
        return fun_eval

    def func_multi(val) :
        x_, y_ = val
        func = sym.poly(x**2 + 2*y**2)
        return eval_(func, [x_, y_]), func

    def func_gradient(fun, val) :
        x_, y_ = val
        _, function = fun(val)
        diff_x = sym.diff(function, x)
        diff_y = sym.diff(function, y)
        grad_vec = np.array([eval_(diff_x, [x_, y_]), eval_(diff_y, [x_, y_])], dtype=float)
        return grad_vec, [diff_x, diff_y]

    def gradient_descent(fun, init_point, lr_rate=0.01, epsilon=0.0001) :
        cnt = 0
        val = init_point
        diff, _ = func_gradient(fun, val)
        while np.linalg.norm(diff) > epsilon :
            val = val - (lr_rate * diff)
            diff, _ = func_gradient(fun, val)
            cnt += 1
        print(f'함수:{fun(val)[1]}, 연산횟수:{cnt},\n최소값:{fun(val)[0]}, xy값:{val}')
    
    random_point = [np.random.uniform(-2,2), np.random.uniform(-2,2)]
    gradient_descent(fun=func_multi, init_point=random_point)
    # 함수:Poly(x**2 + 2*y**2, x, y, domain='ZZ'), 연산횟수:519,
    # 최소값:2.40729579579024E-9, xy값:[-4.90642007e-05  6.79518112e-10]
    ```

##### 경사하강법 II

1.  경사하강법을 통한 선형회귀분석 개요  
    행렬 $\mathbf{X}$에 $m$개의 변수와 $n$개의 샘플이 있고, 각 샘플에 대한 결과값인 $\mathbf{y}$가 있는 상태이다. $\mathbf{X}\beta \approx \mathbf{y}$를 만족하는 $\beta$를 찾고자 하는 것이다.  
    이 때 어떤 $\beta$도 등호를 만족시킬 수는 없으므로, 목적함수를 설정한다. 목적함수는 L2 Norm을 최소화하는 것으로 한다.  
    $\mathbf{X}\beta$는 $\mathbf{y}$를 추정한다는 의미에서 $\hat{\mathbf{y}}$으로 치환할 수 있으므로, 목적식은 다음과 같다. $min_\beta||\mathbf{y} - \hat{\mathbf{y}}||_2$  

2.  목적식과 $\nabla_{\beta}$  
    위 목적식을 따라 $\beta$를 업데이트 하기 위해선 $\beta$에 대한 gradient vector를 구해야 한다. 목적식을 최소화하고자 하며, 이를 위한 기울기는 다음과 같다. $\nabla_{\beta}||\mathbf{y} - \hat{\mathbf{y}}||_2$ 이는 곧 $\nabla_{\beta}||\mathbf{y} - \mathbf{X}\beta||_2$를 의미 한다.  
    $\nabla_{\beta}||\mathbf{y} - \mathbf{X}\beta||_2=(\delta_{\beta_1}||\mathbf{y} - \mathbf{X}\beta||_2, \delta_{\beta_2}||\mathbf{y} - \mathbf{X}\beta||_2, ..., \delta_{\beta_k}||\mathbf{y} - \mathbf{X}\beta||_2)$이다. $k$번째 $\beta$에 대한 미분값을 확인하면 다음과 같다.  
    $\delta_{\beta_k}||\mathbf{y} - \mathbf{X}\beta||_2 = \delta_{\beta_k}\{\frac{1}{n}\sum_{i=1}^{n}( y_i - \sum_{j=1}^{d} \mathbf{X_{ij}}\beta_j)^{2} \}^{\frac{1}{2}}$ 이에 대한 값을 구하면 다음과 같다.  
    $(-\frac{\mathbf{X}_{k}^{T}(\mathbf{y} - \mathbf{X}\beta)}{n||\mathbf{y} - \mathbf{X}\beta||_2})$ 이를 모든 $k$ 즉, $\beta$에 대한 값으로 바꾸면 다음과 같은 식이 완성된다.  
    $\nabla_{\beta}||\mathbf{y} - \mathbf{X}\beta||_2 = -\frac{\mathbf{X}^{T}(\mathbf{y} - \mathbf{X}\beta)}{n||\mathbf{y} - \mathbf{X}\beta||_2}$


3.  $\beta$의 업데이트  
    목적식을 최소화하는 업데이트 식은 다음과 같다. $\beta^{(t+1)} \leftarrow \beta^{(t)} - \lambda\nabla_{\beta}||\mathbf{y} - \mathbf{X}\beta^{(t)}||_2$ 위를 통해 도출된 식을 대입하여 쉽게 계산할 수 있겠다.  
    위의 목적식은 제곱근을 추가한 RMSE이나, MSE의 형태로 계산하여도 동일한 목적을 달성할 수 있다. MSE를 사용하면 식이 간결해지는 효과를 얻을 수 있다.  

4.  경사하강법 기반 선형회귀 알고리즘
    이를 Python code로 나타내면 다음과 같이 나타낼 수 있다.  
    학습종료 조건을 gradient vector의 norm값으로 정할수도 있으나, 근래에는 학습 횟수와 같은 종료조건을 사용하기도 한다. (이와 같은 경우 T, lr 모두 적절한 값을 지정해야 한다.)
    ``` python
    for t in range(T) :  # 종료조건을 학습 횟수로 지정
        error = y - X @ beta  # y에서 yhat을 뺀 값
        grad = - transpose(X) @ error  # 위로부터 도출된 식
        beta = beta - (lr * grad)  # 베타에 대한 업데이트 수행
    ```

5.  경사하강법에 대한 고민  
    위의 알고리즘은 모든 상황에서 작동하지는 않는다. 먼저, 이는 모든 지점에서 미분이 가능해야하며, error에 대한 함수가 볼록한 형태를 가져야 한다. 또한, 비선형적인 모양으로 **지역최소값**이 있는 경우 최소값으로 수렴한다고 보장할 수 없다.  
    이와 같은 약점을 해소하기 위해 **확률적 경사하강법(Stochastic Gradient Descent)** 를 사용한다.

6.  확률적 경사하강법  
    확률적 경사하강법은 매 업데이트마다 일부의 데이터만 사용하는 기법이다. 이를 통해 얻을 수 있는 장점은 다음과 같다.  
    * 연산이 효율적이다. (모든 데이터에 대한 연산을 하지 않기 때문이다.)
    * 지역최소값을 탈출할 수 있다. (매 번 데이터가 달라져서 목적식이 변하기 때문이다.)
    * 훨씬 빠르게 최소점을 찾는다. (연산이 효율적이기 때문이다.)
    * 병렬연산이 가능하다. (업데이트 동안 데이터 전처리 및 계산 준비를 할 수 있다.)
