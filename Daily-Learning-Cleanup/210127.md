## 강의 내용 정리

-   **Pandas I**
-   **Pandas II**
-   **NN's Learning Principle**

##### Pandas I

1.  Pandas란?  
    Pandas는 Panel Data의 줄임말로, 사실상 데이터 처리의 표준 라이브러리로 취급되고 있다. `numpy`기반의 연산과 `R`의 구조체를 차용한 라이브러리이다. Table형태의 데이터를 다룰 때에는 Pandas를 주로 사용할 것이다.  

2.  용어정리  
    Pandas에서 사용하는 데이터 구조의 용어 정리는 다음과 같다.  
    *   모든 데이터 : Data Table
    *   열 : Attribute, Field, Feature, Column
    *   행 : Instance, Tuple, Row

3.  Data load  
    Data를 불러올 때에는 local에 있는 File 혹은 URL을 전달하여 불러올 수 있다.  
    ``` python
    # local data load example
    import pandas as pd
    df = pd.read_csv('./data.csv')

    # web data load example
    url = 'https://...../data'
    df = pd.read_csv(url)
    ```
    불러온 데이터는 `head()`, `describe()`와 같은 메서드를 통해 정보를 확인할 수 있다.  

4.  Series  
    `Series`는 Pandas의 객체이며, 하나의 벡터를 의미한다. 이는 `np.ndarray`의 wrapper class로 비슷한 성질을 지닌다. 단, `Series`는 Index의 중복을 허용한다.
    `Series`를 생성할 때에는 Data에만 값을 전달할 수도 있으며, Dict type의 객체를 사용해 Index도 동시에 생성할수도 있다.  

5.  Series Handling
    `Series`에서 Indexing과 Slicing이 가능하다. `sr_object[index]`와 같은 형태로 사용 가능하며, 해당 index에 대한 값이 출력된다. 반대로 해당하는 값을 재할당하거나 변경하는 것도 가능하다.  
    `sr_object`에는 `values`, `index`, `name`, `index.name`등의 속성이 있고 이를 통해 값을 반환 받을수도, 변경할 수도 있다.  
    `Series`에 새로운 값을 넣으려고 할 때, 값만 준다면 `Default Index`가 생기고, 인덱스만 넣으면 값에는 `NaN`이 들어간다.

6.  DataFrame  
    테이블 형태의 데이터이며, 1개 이상의 `Series`가 모인 형태이다. 각 `column series`는 `dtype`이 다를 수 있다.  
    `df_object`는 `T`, `to_csv()`, `drop()`과 같은 속성과 메서드를 가지고 있다.  

7.  DataFrame Selection  
    `df_object['column_name']`과 같이 사용하면, 해당하는 `column series`에 접근할 수 있다. `df_object[:'column_name']`이나, `df_object[['column_name1','column_name2']]`과 같이 사용하면 해당 column에 해당하는 `dataframe`에 접근할 수 있다.  
    행과 열 모두를 지정해서 접근하고 싶을 때에는 `loc`와 `iloc`를 사용할 수 있다. `loc[]`는 index와 column의 **name**을 지정하는 형식이며, `iloc[]`는 index와 column의 **위치값**을 지정하는 형식으로 사용한다.  
    `boolean index`와 `fancy index`의 방식으로도 접근할 수 있다. 

8.  DataFrame Handling  
    새로운 `column series`를 만들고자 할 때에는 `df_object['new_column']=[values]`와 같은 형태로 할당하면 된다.  
    dataframe에 연산을 가하면 broadcasting이 일어나므로, 조건문의 형태로 boolean 값을 반환 받을 수 있다. `boolean indexing`은 이를 활용한 방법이다.  
    `drop()`메서드는 해당하는 series를 삭제하는데, `axis`의 default 값이 0이다. 즉, 행을 삭제하는 형태가 되며, column series를 삭제하고자 한다면 `axis=1`로 실행해야 한다. `inplace` parameter도 default로 False이기 때문에, 해당 상태로 변경하고자 한다면 `inplace=True`로 실행해야 한다.  
    `del`로 삭제할 수도 있으며, 이는 메모리 상에서 지우는 것이기 때문에 즉시 df_object에 변화가 생긴다.

##### Pandas II

1.  DataFrame Operation  
    두 df_object를 연산하려고 할 때에는 column.name과 index.name이 동일한 데이터끼리 연산이 일어난다. 만약 한쪽에 해당하는 값이 없는 경우에는 NaN으로 처리하고 연산을 수행한다. 이때, 특정 값으로 초기화해서 연산하고자 한다면, `fill_value` parameter에 원하는 값을 입력하면 된다.  
    dataframe 내에서 여러 데이터를 대상으로 하는 연산을 하고자 할때에는 반드시 `axis`를 설정해야 한다. axis가 0인 경우는 행의 값들이 연산되고, axis가 1인 경우는 반대이다.  
    series 혹은 scala 값과 dataframe의 연산이 일어나는 경우에는 broadcasting이 일어나 모든 데이터와 연산을 수행한다.

2.  lambda & map & apply  
    `lambda`식을 통해 만들어진 함수를 모든 데이터에 적용하고자 할 때에 `map()`을 사용할 수 있다. 일반 함수 객체를 만들었을 때에도 동일하다.  
    dict_object를 생성하고, `sr_object.map(dict_object)`를 수행하면, dict_object의 key값과 동일한 인덱스의 값이 dict_object의 값으로 변경된다. sr_object의 인덱스에는 있으나, dict_object에 없는 key인 경우에는 값이 NaN으로 변경된다.  
    `replace()` 메서드는 이와 동일한 기능을 수행한다. Dict type으로 값을 전달해도 되고, 두개의 리스트 형태로 전달할 수도 있다.  
    `apply()` 메서드는 객체에 함수를 적용시키는 것은 동일하나, 모든 데이터에 대해 수행하는 것이 아니라, series 단위로 함수를 수행한다. `df_object.sum()`과 `df_object.apply(sum)`은 동일한 결과값을 반환한다.

3.  Pandas Built-in Function  
    다음과 같은 메서드와 함수가 있으며, 필요시 사용할 수 있겠다.  
    *   df.describe()
    *   df.head()
    *   df.T
    *   sr.sum()
    *   sr.isnull()
    *   sr.unique()
    *   sr.value_counts()
    *   df.sort_values()
    *   df.corr()

##### NN's Learning Principle

1.  비선형적 함수  
    선형모델은 단순 데이터를 해석하는데 유용하나, 복잡한 패턴에는 적합하지 않다. 복잡한 패턴도 근사할 수 있는 비선형적인 함수가 필요하고, 이를 신경망으로 구현할 수 있다.  
    딥러닝. 즉, 신경망은 여러 층의 선형모델과 비선형성을 부여하는 활성함수를 사용하는 합성함수이다.

2.  Forward Propagation I  
    먼저 신경망에 입력되는 Input Data가 있다. 이는 행렬 $\mathbf{X}$로 표현되며, 행렬 내에 있는 각각의 샘플은 벡터인 $\mathbf{x}$로 표현되며, 이 벡터는 $n$개 존재한다. 벡터 $\mathbf{x}$ 내에는 변수 $x_i$가 d개 존재할 것이다. Input data를 정리하면, $\mathbf{X}_(n*d)$가 된다.  
    $d$차원에 있는 벡터 $\mathbf{x}$를 새로운 차원으로 보내기 위해 가중치인 $\mathbf{W}$ 행렬이 존재한다. 이 행렬은 Input data와 내적할 수 있도록 $d$개의 벡터를 가진다. 이 벡터는 목표하는 차원 $p$에 존재한다. 가중치 행렬을 정리하면, $\mathbf{W}_(d*p)$가 된다.  
    각 벡터에 해당하는 절편 $\mathbf{b}$까지 더하게 되면 새로운 행렬 $\mathbf{O}$가 생긴다. 이 행렬의 의미는 $p$ 차원에 존재하는 벡터 $\mathbf{o}$가 $n$개 존재하는 형태가 된다.  
    즉, 가중치 행렬 $\mathbf{W}$과 절편 $\mathbf{b}$를 통해 $d$차원의 데이터 벡터 $\mathbf{x}$가 $p$차원의 데이터 벡터 $\mathbf{o}$가 된 것이다. ($d$개의 변수로 $p$개의 선형모델을 만들어 $p$개의 잠재변수를 만들었다.)  
    여기까지는 Input data에 대한 선형변환에 해당한다.

3.  Forward Propagation II  
    위와 같이 선형변환된 데이터 벡터 $\mathbf{o}$를 비선형함수에 통과시키므로써 비선형적 합성함수가 된다.  
    softmax는 확률벡터를 출력하는 함수로, $softmax(\mathbf{o})$는 특정 클래스 k에 속할 확률 값이 된다. 이외에도 sigmod와 같은 활성함수를 사용할 수 있으며, 활성함수를 적용한 새로운 잠재벡터의 행렬 $\mathbf{H}$를 만들 수 있다.  
    결국 신경망은 **선형모델**과 **활성함수**를 합성한 함수를 의미하는 것이다. 이를 통해 복잡한 분류문제를 해결할 수 있게 된다.  
    단, Forward Propagation은 단순히 입력값을 새로운 형태의 출력값으로 만들뿐이며, 학습은 Back Propagation에 의해 이루어진다.

4.  softmax 구현  
    Python과 numpy를 통해 softmax 함수를 구현한 예시이다. 이때, 지수함수에 전달되는 값이 커져서 overflow가 발생하지 않도록 조정하는 부분을 추가했다.  
    ``` python
    import numpy as np

    def softmax(vec) :
        denumerator = np.exp(vec - np.max(vec, axis=-1, keepdims=True))
        numerator = np.sum(denumerator, axis=-1, keepdims=True)
        val = denumerator / numerator
        return val

    vec = np.array(
        [[1, 2, 0],
        [-1, 0, 1],
        [-10, 0, 10]])
    softmax(vec)
    # output
    # array([[2.44728471e-01, 6.65240956e-01, 9.00305732e-02],
    #       [9.00305732e-02, 2.44728471e-01, 6.65240956e-01],
    #       [2.06106005e-09, 4.53978686e-05, 9.99954600e-01]])
    ```

5.  Activation Function  
    활성함수 **Activation Function**이란, 임의의 실수값을 입력받아서 실수값을 출력하는 비선형함수를 의미한다. 활성함수를 사용하지 않는다면, 아무리 층이 넓고 깊더라도 선형모델에 불과하게 된다.  
    복잡한 패턴과 현상을 파악하기 위해선, 함수의 비선형적 근사가 필수적이다. 대표적으로 XOR게이트와 같은 문제는 단순해보이지만 선형모델로는 해결이 불가능한 문제이다.  
    전통적으로는 sigmoid나 tanh와 같은 활성함수를 사용했으나, 근래에는 ReLU 함수를 많이 사용하고 있다.

6.  Layers  
    위와 같이 Input data를 선형변환한 후, 활성함수를 통해 출력하는 형태를 전통적인 **Perceptron**이라 한다.  
    이렇게 출력된 Output data를 다시 새로운 층의 Input data로 사용하면, 여러 층이 구성될 것이다. 이런 형태를 **Multi Layer Perceptron**이라 한다.  
    신경망을 이렇게 여러 층으로 구성하는 이유는 '넓은' 형태의 신경망이 비효율적이기 때문이다. '얕고 넓게' 신경망을 구축하면, 목적함수에 근사하기 위해 필요한 노드의 수가 기하급수적으로 늘어난다. 실제로 이런 신경망은 잘 작동하지 않는다.  
    층이 깊을수록 훨씬 더 적은 노드와 파라미터로 목적함수에 근사할 수 있다. 하지만, 층이 깊고 복잡한 함수일 수록 학습하기가 어려워지고 최적화가 어려워지기 때문에 적절한 수준을 파악해서 신경망을 구축해야 한다.  

7.  Back Propagation I  
    최종적으로 출력된 Output과 손실함수를 통해 최적화를 시작할 수 있다. 손실함수의 값을 줄이기 위해 파라미터를 최적화하는 작업(학습)을 해야하며, 이를 위해 역전파 알고리즘이 사용된다.  
    경사하강법을 적용하여 각 가중치를 업데이트 해야하며, 선형모델에서 $\beta$의 gradient vector $\nabla\beta$를 구한 것 처럼 각 층에 존재하는 가중치의 미분 값을 구해야한다.  
    미분 값은 한번에 계산되지 않고, chain rule에 의해 상위층부터 하위층으로 순차적으로 계산되어 내려온다. 때문에 이를 **역전파**라 표현한다.

8.  Back Propagation II  
    요새의 Deep Learning Framework에서 역전파 알고리즘은 합성함수 미분법인 chain rule을 기반으로 한 자동미분을 사용한다.  
    각 뉴런은 해당하는 tensor 값을 기억하고 있어야만 미분 계산이 가능하기 때문에, 모든 뉴런에 해당 값을 메모리에 올려놓게 된다. 그렇기 때문에 역전파는 순전파에 비해 메모리를 많이 사용한다.  
    이미 Framework에 구현되어 있는 자동미분 역전파 알고리즘에 의해 현재는 신경망을 구현하여 학습시키면, 주어진 목적식을 최소화하는 파라미터를 구할 수 있게 되었다.

---

## Peer Sesion

-   **강의내용 토의**

##### 강의내용 토의

1.  왜 추론시에는 softmax가 반드시 필요하지 않은가?  
    강의 중 추론 시 softmax 대신 one-hot encoding을 사용할 수 있다고 언급되었다. 이에 대한 질문과 서로의 의견을 공유했다.

2.  sigmoid와 softmax는 확률값인가?  
    Futher Question이자 Peer의 질문이었다. 이에 대한 서로의 의견을 공유했다.

3.  softmax 구현 시 max의 사용 이유?  
    overflow를 막는 원리에 대해 의견을 공유했다.  

4.  비선형적 함수의 의의?  
    비선형적 함수가 어떤 의미를 갖는지 수학적 원리를 기반으로 의견을 공유했다.

---

## 개인 학습

-   **Mathematics for Machine Learning**

##### Mathematics for Machine Learning

1.  선형대수  
    선형대수의 기본 개념과 의의에 대해 공부하고 있다.
2.  행렬  
    왜 역행렬이 중요하고, 행렬의 의미가 무엇인지에 대해 공부하고 있다.